\section{Introduction}
Single-cell omics data, including transcriptomics, proteomics and epigenomics data, provide new opportunities for studying cellular dynamic processes, such as the cell cycle, cell differentiation and cell activation \cite{tanay_scalingsinglecellgenomics_2017,etzrodt_quantitativesinglecellapproaches_2014}. Such dynamic processes can be modeled computationally using trajectory inference (TI) methods, also called pseudotime analysis, which order cells along a trajectory based on similarities in their expression patterns \cite{trapnell_definingcelltypes_2015,cannoodt_computationalmethodstrajectory_2016,moon_manifoldlearningbasedmethods_2018}. The resulting trajectories are most often linear, bifurcating or tree-shaped, but more recent methods also identify more complex trajectory topologies, such as cyclic \cite{liu_reconstructingcellcycle_2017} or disconnected graphs \cite{wolf_graphabstractionreconciles_2017}. TI methods offer an unbiased and transcriptome-wide understanding of a dynamic process\cite{tanay_scalingsinglecellgenomics_2017}, thereby allowing the objective identification of new (primed) subsets of cells \cite{schlitzer_identificationcdc1cdc2committed_2015}, delineation of a differentiation tree \cite{velten_humanhaematopoieticstem_2017,see_mappinghumandc_2017} and inference of regulatory interactions responsible for one or more bifurcations \cite{aibar_scenicsinglecellregulatory_2017}. Current applications of TI focus on specific subsets of cells, but ongoing efforts to construct transcriptomic catalogs of whole organisms \cite{regev_humancellatlas_2017,han_mappingmousecell_2018,schaum_singlecelltranscriptomics20_2018} underline the urgency for accurate, scalable \cite{aibar_scenicsinglecellregulatory_2017,angerer_singlecellsmake_2017} and user-friendly TI methods.

A plethora of TI methods has been developed over the past few years and even more are being created every month (Supplementary Table 1). Indeed, in several repositories listing single-cell tools, such as omictools.org \cite{henry_omictoolsinformativedirectory_2014}, the ‘awesome-single-cell’ list \cite{davis_awesomesinglecell_2018} and scRNA-tools.org \cite{zappia_exploringsinglecellrnaseq_2017}, TI methods are one of the largest categories. While each method has its own unique set of characteristics in terms of underlying algorithm, required prior information and produced outputs, two of the most distinctive differences between TI methods are whether they fix the topology of the trajectory and what type(s) of graph topologies they can detect. Early TI methods typically fixed the topology algorithmically (for example, linear \cite{bendall_singlecelltrajectorydetection_2014,schlitzer_identificationcdc1cdc2committed_2015,shin_singlecellrnaseqwaterfall_2015,campbell_bayesiangaussianprocess_2015} or bifurcating trajectories \cite{haghverdi_diffusionpseudotimerobustly_2016,setty_wishboneidentifiesbifurcating_2016}) or through parameters provided by the user \cite{trapnell_dynamicsregulatorscell_2014,matsumoto_scoupprobabilisticmodel_2016}. These methods therefore mainly focus on correctly ordering the cells along the fixed topology. More recent methods also infer the topology \cite{qiu_reversedgraphembedding_2017,street_slingshotcelllineage_2018,wolf_graphabstractionreconciles_2017}, which increases the difficulty of the problem at hand, but allows the unbiased identification of both the ordering inside a branch and the topology connecting these branches.

Given the diversity in TI methods, it is important to quantitatively assess their performance, scalability, robustness and usability. Many attempts at tackling this issue have already been made \cite{haghverdi_diffusionpseudotimerobustly_2016,ji_tscanpseudotimereconstruction_2016,welch_slicerinferringbranched_2016,matsumoto_scoupprobabilisticmodel_2016,duverle_celltreebioconductorpackage_2016,cannoodt_computationalmethodstrajectory_2016,lonnberg_singlecellrnaseqcomputational_2017,campbell_probabilisticmodelingbifurcations_2017,wolf_graphabstractionreconciles_2017}, but a comprehensive comparison of TI methods across a large number of different datasets is still lacking. This is problematic, as new users to the field are confronted with an overwhelming choice of TI methods, without a clear idea of which would optimally solve their problem. Moreover, the strengths and weaknesses of existing methods need to be assessed, so that new developments in the field can focus on improving the current state-of-the-art.

In this study, we evaluated the accuracy, scalability, stability and usability of 45 TI methods (Figure \ref{fig:figure_1}a). We found substantial complementarity between current methods, with different sets of methods performing most optimally depending on the characteristics of the data. For method users, we created an interactive set of guidelines (available at \href{http://guidelines.dynverse.org}{guidelines.dynverse.org}), which gives context-specific recommendations for method usage. Our evaluation also highlights some challenges for current methods, and our evaluation strategy can be useful to spearhead the development of new tools that accurately infer trajectories on ever more complex use cases.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_1.pdf}
	\caption{
		\textbf{Overview of several key aspects of the evaluation.}
		\textbf{a}, A schematic overview of our evaluation pipeline. \textbf{b}, To make the trajectories comparable to each other, a common trajectory model was used to represent reference trajectories from the real and synthetic datasets, as well as any predictions of TI methods. \textbf{c}, Trajectories are automatically classified into one of seven trajectory types, with increasing complexity. \textbf{d}, We defined four metrics, each assessing the quality of a different aspect of the trajectory. The HIM score assesses the similarity between the two topologies, taking into account differences in edge lengths and degree distributions. The F1branches assesses the similarity of the assignment of cells onto branches. The cordist quantifies the similarity in cellular positions between two trajectories, by calculating the correlation between pairwise geodesic distances. Finally, wcorfeatures quantifies the agreement between trajectory differentially expressed features from the known trajectory and the predicted trajectory.
	}
	\label{fig:figure_1}
\end{figure}

\section{Results}

\subsection{Trajectory inference methods}
To make the outputs from different methods directly comparable to each other, we developed a common probabilistic model for representing trajectories from all possible sources (Figure \ref{fig:figure_1}b). In this model, the overall topology is represented by a network of ‘milestones’, and the cells are placed within the space formed by each set of connected milestones. Although almost every method returned a unique set of outputs, we were able to classify these outputs into seven distinct groups (Figure \ref{fig:supfigure_1}) and we wrote a common output converter for each of these groups (Figure \ref{fig:figure_2}a). When strictly required, we also provided prior information to the method. These different priors can range from weak priors that are relatively easy to acquire, such as a start cell, to strong priors, such as a known grouping of cells, that are much harder to know a priori, and which can potentially introduce a large bias into the analysis (Figure \ref{fig:figure_2}a).

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/supfigure_1.pdf}
	\caption{
		\textbf{A common interface for TI methods.}
		\textbf{a} The input and output of each TI method is standardized. As input, each TI method receives either raw or normalized counts, several parameters, and a selection of prior information. After its execution, a method uses one of the seven wrapper functions to transform its output to the common trajectory model. This common model then allows to perform common analysis functions on trajectory models produced by any TI method. \textbf{b} Illustrations of the specific transformations performed by each of the wrapper functions.
	}
	\label{fig:supfigure_1}
\end{figure}

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_2.pdf}
	\caption{
		\textbf{A characterization of the 45 methods evaluated in this study and their overall evaluation results.}
		\textbf{a}, We characterized the methods according to the wrapper type, their required priors, whether the inferred topology is constrained by the algorithm (fixed) or a parameter (param), and the types of inferable topologies. The methods are grouped vertically based on the most complex trajectory type they can infer. \textbf{b}, The overall results of the evaluation on four criteria: accuracy using a reference trajectory on real and synthetic data, scalability with increasing number of cells and features, stability across dataset subsamples and quality of the implementation. Methods that errored on more than 50$\%$ of the datasets are not included in this figure and are shown instead in Supplementary Fig. 2.
	}
	\label{fig:figure_2}
\end{figure}

The largest difference between TI methods is whether a method fixes the topology and, if it does not, what kind of topology it can detect. We defined seven possible types of topology, ranging from very basic topologies (linear, cyclical and bifurcating) to the more complex ones (connected and disconnected graphs). Most methods either focus on inferring linear trajectories or limit the search to tree or less complex topologies, with only a selected few attempting to infer cyclic or disconnected topologies (Figure \ref{fig:figure_2}a).

We evaluated each method on four core aspects: (1) accuracy of a prediction, given a gold or silver standard on 110 real and 229 synthetic datasets; (2) scalability with respect to the number of cells and features (for example, genes); (3) stability of the predictions after subsampling the datasets; and (4) the usability of the tool in terms of software, documentation and the manuscript. Overall, we found a large diversity across the four evaluation criteria, with only a few methods, such as PAGA, Slingshot and SCORPIUS, performing well across the board (Figure \ref{fig:figure_2}b). We will discuss each evaluation criterion in more detail (Figure \ref{fig:figure_3} and Supplementary Fig. 2), after which we conclude with guidelines for method users and future perspectives for method developers.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_3.pdf}
	\caption{	
		\textbf{Detailed results of the four main evaluation criteria: accuracy, scalability, stability and usability.}
		\textbf{a,} The names of the methods, ordered as in Figure \ref{fig:figure_2}. \textbf{b}, Accuracy of trajectory inference methods across metrics, dataset sources and dataset trajectory types. The performance of a method is generally more stable across dataset sources, but very variable depending on the metric and trajectory type. \textbf{c}, Predicted execution times for varying numbers of cells and features (no. of cells $\times$ no. of features). Predictions were made by training a regression model after running each method on bootstrapped datasets with varying numbers of cells and features. k, thousands; m, millions; cor, correlation. \textbf{d}, Stability results by calculating the average pairwise similarity between models inferred across multiple runs of the same method. \textbf{e}, Usability scores of the tool and corresponding manuscript, grouped per category. Off-the-shelf methods were directly implemented in R and thus do not have a usability score. 
	}
	\label{fig:figure_3}
\end{figure}

\subsection{Accuracy}

We defined several metrics to compare a prediction to a reference trajectory (Supplementary Note 1). Based on an analysis of their robustness and conformity to a set of rules (Supplementary Note 1), we chose four metrics each assessing a different aspect of a trajectory (Figure \ref{fig:figure_1}d): the topology (Hamming–Ipsen–Mikhailov, HIM), the quality of the assignment of cells to branches (F1branches), the cell positions (cordist) and the accuracy of the differentially expressed features along the trajectory (wcorfeatures). The data compendium consisted of both synthetic datasets, which offer the most exact reference trajectory, and real datasets, which provide the highest biological relevance. These real datasets come from a variety of single-cell technologies, organisms and dynamic processes, and contain several types of trajectory topologies (Supplementary Table 2). Real datasets were classified as ‘gold standard’ if the reference trajectory was not extracted from the expression data itself, such as via cellular sorting or cell mixing \cite{tian_scrnaseqmixologybetter_2018}. All other real datasets were classified as ‘silver standard’. For synthetic datasets we used several data simulators, including a simulator of gene regulatory networks using a thermodynamic model of gene regulation \cite{schaffter_genenetweaversilicobenchmark_2011}. For each simulation, we used a real dataset as a reference, to match its dimensions, number of differentially expressed genes, drop-out rates and other statistical properties \cite{zappia_splattersimulationsinglecell_2017}.

We found that method performance was very variable across datasets, indicating that there is no ‘one-size-fits-all’ method that works well on every dataset (Figure \ref{fig:supfigure_3}a). Even methods that can detect most of the trajectory types, such as PAGA, RaceID/StemID and SLICER were not the best methods across all trajectory types (Figure \ref{fig:figure_3}b). The overall score between the different dataset sources was moderately to highly correlated (Spearman rank correlation between 0.5–0.9) with the scores on real datasets containing a gold standard (Figure \ref{fig:supfigure_3}b), confirming both the accuracy of the gold standard trajectories and the relevance of the synthetic data. On the other hand, the different metrics frequently disagreed with each other, with Monocle and PAGA Tree scoring better on the topology scores, whereas other methods, such as Slingshot, were better at ordering the cells and placing them into the correct branches (Figure \ref{fig:figure_3}b).

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/supfigure_3.pdf}
	\caption{
		\textbf{Accuracy of trajectory inference methods.}
		\textbf{a} Overall score for all methods across 339 datasets, colored by the source of the datasets. Black line indicates the mean. \textbf{b} Similarity between the overall scores of all dataset sources, compared to real datasets with a gold standard, across all methods (n = 46, after filtering out methods that errored too frequently). Shown in the top left is the Pearson correlation. \textbf{c} Bias in the overall score towards trajectory types for all methods across 339 datasets. Black line indicates the mean. \textbf{d} Distributions of the difference in size between predicted and reference topologies. A positive difference means that the topology predicted by the method is more complex than the one in the reference.
	}
	\label{fig:supfigure_3}
\end{figure}



The performance of a method was strongly dependent on the type of trajectory present in the data (Figure \ref{fig:figure_3}b). Slingshot typically performed better on datasets containing more simple topologies, while PAGA, pCreode and RaceID/StemID had higher scores on datasets with trees or more complex trajectories (Figure \ref{fig:supfigure_3}c). This was reflected in the types of topologies detected by every method, as those predicted by Slingshot tended to contain less branches, whereas those detected by PAGA, pCreode and Monocle DDRTree gravitated towards more complex topologies (Figure \ref{fig:supfigure_3}d). This analysis therefore indicates that detecting the right topology is still a difficult task for most of these methods, because methods tend to be either too optimistic or too pessimistic regarding the complexity of the topology in the data.

The high variability between datasets, together with the diversity in detected topologies between methods, could indicate some complementarity between the different methods. To test this, we calculated the likelihood of obtaining a top model when using only a subset of all methods. A top model in this case was defined as a model with an overall score of at least 95$\%$ as the best model. On all datasets, using one method resulted in getting a top model about 27$\%$ of the time. This increased up to 74$\%$ with the addition of six other methods (Figure \ref{fig:figure_4}a). The result was a relatively diverse set of methods, containing both strictly linear or cyclic methods, and methods with a broad trajectory type range such as PAGA. We found similar indications of complementarity between the top methods on data containing only linear, bifurcation or multifurcating trajectories (Figure \ref{fig:figure_4}b), although in these cases less methods were necessary to obtain at least one top model for a given dataset. Altogether, this shows that there is considerable complementarity between the different methods and that users should try out a diverse set of methods on their data, especially when the topology is unclear a priori. Moreover, it also opens up the possibilities for new ensemble methods that utilize this complementarity.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_4.pdf}
	\caption{
		\textbf{Complementarity between different trajectory inference methods.}
		\textbf{a}, We assessed the likelihood for different combinations of methods to lead to a ‘top model’ (defined as a model with an overall score of at least 95$\%$ of the best model) when applied to all datasets. \textbf{b}, The likelihood for different combinations of methods to lead to a ‘top model’ was assessed separately on different trajectory types. For this figure, we did not include any methods requiring a cell grouping or a time course as prior information.
	}
	\label{fig:figure_4}
\end{figure}

\subsection{Scalability}

While early TI methods were developed at a time where profiling more than a thousand cells was exceptional, methods now have to cope with hundreds of thousands of cells, and perhaps soon with more than ten million \cite{svensson_exponentialscalingsinglecell_2018}. Moreover, the recent application of TI methods on multi-omics single-cell data also showcases the increasing demands on the number of features \cite{cao_jointprofilingchromatin_2018}. To assess the scalability, we ran each method on up- and downscaled versions of five distinct real datasets. We modeled the running time and memory usage using a Shape Constrained Additive Model \cite{pya_shapeconstrainedadditive_2015} (Figure \ref{fig:supfigure_4}a). As a control, we compared the predicted time (and memory) with the actual time (respectively memory) on all benchmarking datasets, and found that these were highly correlated overall (Spearman rank correlation >0.9, Supplementary Fig. 5), and moderately to highly correlated (Spearman rank correlation of 0.5–0.9) for almost every method, depending to what extent the execution of a method succeeded during the scalability experiments (Figure \ref{fig:figure_3}c and Supplementary Fig. 2a).

\begin{figure}[htb!]
	\centering\includegraphics[width=0.9\linewidth]{fig/supfigure_4.pdf}
	\caption{
		\textbf{Scalability of trajectory inference methods.}
		\textbf{a} Three examples of average observed running times across five datasets (left) and the predicted running time (right). \textbf{b} Overview of the scalability results of all methods, ordered by their average predicted running time from (a). We predicted execution times and memory usage for each method with increasing number of features or cells, and used these values to classify each method into sublinear, linear, quadratic and superquadratic based on the shape of the curve.
	}
	\label{fig:supfigure_4}
\end{figure}

We found that the scalability of most methods was overall very poor, with most graph and tree methods not finishing within an hour on a dataset with ten thousand cells and ten thousand features (Figure \ref{fig:figure_3}c), which is around the size of a typical droplet-based single-cell dataset \cite{svensson_exponentialscalingsinglecell_2018}. Running times increased further with increasing number of cells, with only a handful of graph/tree methods completing within a day on a million cells (PAGA, PAGA Tree, Monocle DDRTree, Stemnet and GrandPrix). Some methods, such as Monocle DDRTree and GrandPrix, also suffered from unsatisfactory running times when given a high number of features.

Methods with a low running time typically had two defining aspects: they had a linear time complexity with respect to the features and/or cells, and adding new cells or features led to a relatively low increase in time (Figure \ref{fig:supfigure_4}b). We found that more than half of all methods had a quadratic or superquadratic complexity with respect to the number of cells, which would make it difficult to apply any of these methods in a reasonable time frame on datasets with more than a thousand cells (Figure \ref{fig:supfigure_4}b).

We also assessed the memory requirements of each method (Supplementary Fig. 2c). Most methods had reasonable memory requirements for modern workstations or computer clusters ($\leq$12 GB) with PAGA and STEMNET in particular having a low memory usage with both a high number of cells or a high number of features. Notably, the memory requirements were very high for several methods on datasets with high numbers of cells (RaceID/StemID, pCreode and MATCHER) or features (Monocle DDRTree, SLICE and MFA).

Altogether, the scalability analysis indicated that the dimensions of the data are an important factor in the choice of method, and that method development should pay more attention to maintaining reasonable running times and memory usage.
§
\subsection{Stability}

It is not only important that a method is able to infer an accurate model in a reasonable time frame, but also that it produces a similar model when given very similar input data. To test the stability of each method, we executed each method on ten different subsamples of the datasets (95$\%$ of the cells, 95$\%$ of the features), and calculated the average similarity between each pair of models using the same scores used to assess the accuracy of a trajectory (Figure \ref{fig:figure_3}d).

Given that the trajectories of methods that fix the topology either algorithmically or through a parameter are already very constrained, it is to be expected that such methods tend to generate very stable results. Nonetheless, some fixed topology methods still produced slightly more stable results, such as SCORPIUS and MATCHER for linear methods and MFA for multifurcating methods. Stability was much more diverse among methods with a free topology. Slingshot produced more stable models than PAGA (Tree), which in turn produced more stable results than pCreode and Monocle DDRTree.

\subsection{Usability}

While not directly related to the accuracy of the inferred trajectory, it is also important to assess the quality of the implementation and how user-friendly it is for a biological user \cite{taschuk_tensimplerules_2017}. We scored each method using a transparent checklist of important scientific and software development practices, including software packaging, documentation, automated code testing and publication into a peer-reviewed journal (Table \ref{tab:scoresheet}). It is important to note that there is a selection bias in the tools chosen for this analysis, as we did not include a substantial set of tools due to issues with installation, code availability and executability on a freely available platform (which excludes MATLAB). The reasons for not including certain tools are all discussed on our repository (https://github.com/dynverse/dynmethods/issues?q=label:unwrappable). Installation issues seem to be quite general in bioinformatics \cite{mangul_comprehensiveanalysisusability_2018} and the trajectory inference field is no exception.

We found that most methods fulfilled the basic criteria, such as the availability of a tutorial and elemental code quality criteria (Figure \ref{fig:figure_3}d and Supplementary Fig. 6). While recent methods had a slightly better quality score than older methods, several quality aspects were consistently lacking for the majority of the methods (Supplementary Fig. 6 right) and we believe that these should receive extra attention from developers. Although these outstanding issues covered all five categories, code assurance and documentation in particular were problematic areas, notwithstanding several studies pinpointing these as good practices \cite{wilson_bestpracticesscientific_2014,artaza_top10metrics_2016}. Only two methods had a nearly perfect usability score (Slingshot and Celltrails), and these could be used as an inspiration for future methods. We observed no clear relation between usability and method accuracy or usability (Figure \ref{fig:figure_2}b).



\begin{table}
	\caption{\textbf{Scoring sheet for assessing usability of trajectory inference methods.} Each quality aspect was given a weight based on how many times it was mentioned in a set of articles discussing best practices for tool development.} \label{tab:scoresheet}
	
	\centering\begingroup\fontsize{7}{9}\selectfont
	
	\begin{tabular}{p{3.5cm}p{9cm}p{2.5cm}}
		\toprule
		Aspect & Items & References \\ \midrule
		
		\multicolumn{3}{l}{\textbf{Availability}}\\*
		
		Open source & (1) Method's code is freely available (2) The code can be run on a freely available platform & \cite{lee_rpackagespackagedevelopment_2017,wilson_bestpracticesscientific_2014,taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,silva_generalguidelinesbiomedical_2017,jimenez_foursimplerecommendations_2017}\\*
		
		Version control & The code is available on a public version controlled repository, such as Github & \cite{lee_rpackagespackagedevelopment_2017,wilson_bestpracticesscientific_2014,taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,silva_generalguidelinesbiomedical_2017}\\*
		
		Packaging & (1) The code is provided as a "package", exposing functionality through functions or shell commands (2) The code can be easily installed through a repository such as CRAN, Bioconductor, PyPI, CPAN, debian packages, … & \cite{lee_rpackagespackagedevelopment_2017,wickham_packagesorganizetest_2015,jimenez_foursimplerecommendations_2017,silva_generalguidelinesbiomedical_2017}\\*
		
		Dependencies & (1) Dependencies are clearly stated in the tutorial or in the code (2) Dependencies are automatically installed & \cite{taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,karimzadeh_topconsiderationscreating_}\\*
		
		License & (1) The code is licensed (2) License allows academic use & \cite{lee_rpackagespackagedevelopment_2017,taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,silva_generalguidelinesbiomedical_2017,jimenez_foursimplerecommendations_2017}\\*
		
		Interface & (1) The tool can be run using a graphical user interface, either locally or on a web server (2) The tool can be run through the command line or through a programming language & \cite{silva_generalguidelinesbiomedical_2017}\\
		
		\midrule
		\multicolumn{3}{l}{\textbf{Code quality}} \\*
		
		Function and object naming & (1) Functions/commands have well chosen names (2) Arguments/parameters have well chosen names & \cite{wilson_bestpracticesscientific_2014,wickham_packagesorganizetest_2015}\\*
		
		Code style & (1) Code has a consistent style (2) Code follows (basic) good practices in the programming language of choice, for example PEP8 or the tidyverse style guide & \cite{wilson_bestpracticesscientific_2014,wickham_packagesorganizetest_2015,artaza_top10metrics_2016}\\*
		
		Code duplication & Duplicated code is minimal & \cite{wilson_bestpracticesscientific_2014,wickham_packagesorganizetest_2015}\\
		
		Self-contained functions & The method is exposed to the user as self-contained functions or commands & \cite{anderson_writinggreatscientific_2016,taschuk_tensimplerules_2017,silva_generalguidelinesbiomedical_2017}\\*
		
		Plotting & Plotting functions are provided for the final and/or intermediate results & \\*
		
		Dummy proofing & Package contains dummy proofing, i.e. testing whether the parameters and data supplied by the user make sense and are useful & \cite{lee_rpackagespackagedevelopment_2017,karimzadeh_topconsiderationscreating_}\\
		
		\midrule
		\multicolumn{3}{l}{\textbf{Code assurance}} \\*
		
		Unit testing & Method is tested using unit tests & \cite{lee_rpackagespackagedevelopment_2017,wilson_bestpracticesscientific_2014,anderson_writinggreatscientific_2016,wickham_packagesorganizetest_2015,silva_generalguidelinesbiomedical_2017}\\*
		
		Unit testing & Tests are run automatically using functionality from the programming language & \cite{lee_rpackagespackagedevelopment_2017,wilson_bestpracticesscientific_2014,anderson_writinggreatscientific_2016,wickham_packagesorganizetest_2015,silva_generalguidelinesbiomedical_2017}\\*
		
		Continuous integration & The method uses continuous integration, for example on Travis CI & \cite{beaulieu-jones_reproducibilitycomputationalworkflows_2017,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,silva_generalguidelinesbiomedical_2017}\\*
		
		Code coverage & (1) The code coverage of the repository is assessed. (2) What is the percentage of code coverage & \\
		
		\midrule
		\multicolumn{3}{l}{\textbf{Documentation}} \\*
		
		Support & (1) There is a support ticket system, for example on Github (2) The authors respond to tickets and issues are resolved within a reasonable time frame & \cite{wilson_bestpracticesscientific_2014,wickham_packagesorganizetest_2015,artaza_top10metrics_2016,silva_generalguidelinesbiomedical_2017,jimenez_foursimplerecommendations_2017}\\*
		
		Development model & (1) The repository separates the development code from master code, for example using git master en developer branches (2) The repository has created releases, or several branches corresponding to major releases. (3) The repository has branches for the development of separate features. & \cite{driessen_successfulgitbranching_2010}\\*
		
		Tutorial & (1) A tutorial or vignette is available (2) The tutorial has example results (3) The tutorial has real example data (4) The tutorial showcases the method on several datasets (1=0, 2=0.5, >2=1) & \cite{wickham_packagesorganizetest_2015,silva_generalguidelinesbiomedical_2017,jimenez_foursimplerecommendations_2017,karimzadeh_topconsiderationscreating_,boulesteix_tensimplerules_2015}\\*
		
		Function documentation & (1) The purpose and usage of functions/commands is documented (2) The parameters of functions/commands are documented (3) The output of functions/commands is documented & \cite{wilson_bestpracticesscientific_2014,taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,silva_generalguidelinesbiomedical_2017,karimzadeh_topconsiderationscreating_}\\*
		
		Inline documentation & Inline documentation is present in the code & \cite{wilson_bestpracticesscientific_2014,taschuk_tensimplerules_2017,wickham_packagesorganizetest_2015,silva_generalguidelinesbiomedical_2017,karimzadeh_topconsiderationscreating_}\\*
		
		Parameter transparency & All important parameters are exposed to the user & \cite{taschuk_tensimplerules_2017}\\
		
		\midrule
		\multicolumn{3}{l}{\textbf{Behaviour}} \\*
		
		Seed setting & The method does not artificially become deterministic (1), for example by setting some (0.5) or a lot (0) of seeds & \cite{puget_greendiceare_2016}\\*
		
		Unexpected output & (1) No unexpected output messages are generated by the method (2) No unexpected files, folders or plots are generated (3) No unexpected warnings during runtime or compilation are generated & \cite{artaza_top10metrics_2016}\\*
		
		Trajectory format & The postprocessing necessary to extract the relevant output from the method is minimal (1), moderate (0.5) or extensive (0) & \\*
		
		Prior information & Prior information is required (0), optional (1) or not required (1) & \\*
		
		\midrule
		\multicolumn{3}{l}{\textbf{Paper}} \\*
		
		Publishing & The method is published & \\*
		
		Peer review & The paper is published in a peer-reviewed journal & \cite{karimzadeh_topconsiderationscreating_,gannon_essentialrolepeer_2001,baldwin_refereeswetrust_2017}\\*
		
		Evaluation on real data & (1) The paper shows the method's usefulness on several (1), one (0.25) or no real datasets. (2) The paper quantifies the accuracy of the method given a gold or silver standard trajectory & \cite{aniba_issuesbioinformaticsbenchmarking_2010,jelizarow_overoptimismbioinformaticsillustration_2010}\\*
		
		Evaluation of robustness & The paper assessed method robustness (to eg. noise, subsampling, parameter changes, stability) in one (0.5) or several (1) ways & \cite{karimzadeh_topconsiderationscreating_,aniba_issuesbioinformaticsbenchmarking_2010,boulesteix_tensimplerules_2015,jelizarow_overoptimismbioinformaticsillustration_2010}\\
		
		\bottomrule
	\end{tabular}
	\endgroup{}
\end{table}

\section{Discussion}

In this study, we presented a large-scale evaluation of the performance of 45 TI methods. By using a common trajectory representation and four metrics to compare the methods’ outputs, we were able to assess the accuracy of the methods on more than 200 datasets. We also assessed several other important quality measures, such as the quality of the method’s implementation, the scalability to hundreds of thousands of cells and the stability of the output on small variations of the datasets.

Based on the results of our benchmark, we propose a set of practical guidelines for method users (Figure \ref{fig:figure_5} and guidelines.dynverse.org). We postulate that, as a method’s performance is heavily dependent on the trajectory type being studied, the choice of method should currently be primarily driven by the anticipated trajectory topology in the data. For most use cases, the user will know very little about the expected trajectory, except perhaps whether the data is expected to contain multiple disconnected trajectories, cycles or a complex tree structure. In each of these use cases, our evaluation suggests a different set of optimal methods, as shown in Figure \ref{fig:figure_5}. Several other factors will also impact the choice of methods, such as the dimensions of the dataset and the prior information that is available. These factors and several others can all be dynamically explored in our interactive app (guidelines.dynverse.org). This app can also be used to query the results of this evaluation, such as filtering the datasets or changing the importance of the evaluation metrics for the final ranking.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_5.pdf}
	\caption{
		\textbf{Practical guidelines for method users.}
		As the performance of a method mostly depends on the topology of the trajectory, the choice of TI method will be primarily influenced by the user’s existing knowledge about the expected topology in the data. We therefore devised a set of practical guidelines, which combines the method’s performance, user friendliness and the number of assumptions a user is willing to make about the topology of the trajectory. Methods to the right are ranked according to their performance on a particular (set of) trajectory type. Further to the right are shown the accuracy (+: scaled performance $\geq$ 0.9, $\pm$: >0.6), usability scores (+:$\geq$0.9, $\pm$ $\geq$0.6), estimated running times and required prior information. k, thousands; m, millions.
	}
	\label{fig:figure_5}
\end{figure}

When inferring a trajectory on a dataset of interest, it is important to take two further points into account. First, it is critical that a trajectory, and the downstream results and/or hypotheses originating from it, are confirmed by multiple TI methods. This is to make sure that the prediction is not biased due to the given parameter setting or the particular algorithm underlying a TI method. The value of using different methods is further supported by our analysis indicating substantial complementarity between the different methods. Second, even if the expected topology is known, it can be beneficial to also try out methods that make less assumptions about the trajectory topology. When the expected topology is confirmed using such a method, it provides additional evidence to the user. When a more complex topology is produced, this could indicate that the underlying biology is much more complex than anticipated by the user.

Critical to the broad applicability of TI methods is the standardization of the input and output interfaces of TI methods, so that users can effortlessly execute TI methods on their dataset of interest, compare different predicted trajectories and apply downstream analyses, such as finding genes important for the trajectory, network inference \cite{aibar_scenicsinglecellregulatory_2017} or finding modules of genes \cite{saelens_comprehensiveevaluationmodule_2018}. Our framework is an initial attempt at tackling this problem, and we illustrate its usefulness here by comparing the predicted trajectories of several top-performing methods on datasets containing a linear, tree, cyclic and disconnected graph topology (Figure \ref{fig:figure_6}). Using our framework, this figure can be recreated using only a couple of lines of R code (https://methods.dynverse.org). In the future, this framework could be extended to allow additional input data, such as spatial and RNA velocity information \cite{manno_rnavelocitysingle_2018}, and easier downstream analyses. In addition, further discussion within the field is required to arrive at a consensus concerning a common interface for trajectory models, which can include additional features such as uncertainty and gene importance.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/figure_6.pdf}
	\caption{
		\textbf{Demonstration of how a common framework for TI methods facilitates broad applicability using some example datasets.}
		Trajectories inferred by each method were projected to a common dimensionality reduction using multidimensional scaling. For each dataset, we also calculated a ‘consensus’ prediction, by calculating the cordist between each pair of models and picking the model with the highest score on average. \textbf{a}, The top methods applied on a dataset containing a linear trajectory of differentiation dendritic cells, going from MDP, CDP to PreDC. \textbf{b}, The top methods applied on a dataset containing a bifurcating trajectory of reprogrammed fibroblasts. \textbf{c}, A synthetic dataset generated by dyntoy, containing four disconnected trajectories. \textbf{d}, A synthetic dataset generated by dyngen, containing a cyclic trajectory.
	}
	\label{fig:figure_6}
\end{figure}

Our study indicates that the field of trajectory inference is maturing, primarily for linear and bifurcating trajectories (Figure \ref{fig:figure_6}a,b). However, we also highlight several ongoing challenges, which should be addressed before TI can be a reliable tool for analyzing single-cell omics datasets with complex trajectories. Foremost, new methods should focus on improving the unbiased inference of tree, cyclic graph and disconnected topologies, as we found that methods repeatedly overestimate or underestimate the complexity of the underlying topology, even if the trajectory could easily be identified using a dimensionality reduction method (Figure \ref{fig:figure_6}c,d). Furthermore, higher standards for code assurance and documentation could help in adopting these tools across the single-cell omics field. Finally, new tools should be designed to scale well with the increasing number of cells and features. We found that only a handful of current methods can handle datasets with more than 10,000 cells within a reasonable time frame. To support the development of these new tools, we provide a series of vignettes on how to wrap and evaluate a method on the different measures proposed in this study at \href{https://benchmark.dynverse.org}{https://benchmark.dynverse.org}.

We found that the performance of a method can be very variable between datasets, and therefore included a large set of both real and synthetic data within our evaluation, leading to a robust overall ranking of the different methods. However, ‘good-yet-not-the-best’ methods \cite{norel_selfassessmenttrapcan_2011} can still provide a very valuable contribution to the field, especially if they make use of novel algorithms, return a more scalable solution or provide a unique insight in specific use cases. This is also supported by our analysis of method complementarity. Some examples for the latter include PhenoPath, which can include additional covariates in its model, ouija, which returns a measure of uncertainty of each cell’s position within the trajectory, and StemID, which can infer the directionality of edges within the trajectory.


\section{Methods}

\subsection{Trajectory inference methods}

We gathered a list of 71 trajectory inference tools (Supplementary Table 1) by searching the literature for ‘trajectory inference’ and ‘pseudotemporal ordering’, and based on two existing lists found online: \href{https://github.com/seandavi/awesome-single-cell}{awesome-single-cell} \cite{davis_awesomesinglecell_2018} and \href{https://github.com/agitter/single-cell-pseudotime}{single-cell-pseudotime} \cite{gitter_singlecellrnaseqpseudotime_2018}. We welcome any contributions by creating an issue at \href{https://methods.dynverse.org}{https://methods.dynverse.org}.

Methods were excluded from the evaluation based on several criteria: (1) not freely available; (2) no code available; (3) superseded by another method; (4) requires data types other than expression; (5) no programming interface; (6) unresolved errors during wrapping; (7) too slow (requires more than 1 h on a 100 $\times$ 100 dataset); (8) does not return an ordering; and (9) requires additional user input during the algorithm (other than prior information). The discussions on why these methods were excluded can be found at \href{https://github.com/dynverse/dynmethods/issues?q=label:unwrappable}{https://github.com/dynverse/dynmethods/issues?q=label:unwrappable}. In the end, we included 45 methods in the evaluation.


\subsection{Method wrappers}

To make it easy to run each method in a reproducible manner, each method was wrapped within Docker and singularity containers (available at \href{https://methods.dynverse.org}{https://methods.dynverse.org}). These containers are automatically built and tested using Travis continuous integration (\href{https://travis-ci.org/dynverse}{https://travis-ci.org/dynverse}) and can be ran using both Docker and Singularity. For each method, we wrote a wrapper script based on example scripts or tutorials provided by the authors (as mentioned in the respective wrapper scripts). This script reads in the input data, runs the method and outputs the files required to construct a trajectory. We also created a script to generate an example dataset, which is used for automated testing.

We used the Github issues system to contact the authors of each method, and asked for feedback on the wrappers, the metadata and the usability scores. About one-third of the authors responded and we improved the wrappers based on their feedback. These discussions can be viewed on Github: \href{https://github.com/dynverse/dynmethods/issues?q=label:method_discussion}{https://github.com/dynverse/dynmethods/issues?q=label:method\_discussion}

\subsubsection{Method input}

As input, we provided each method with either the raw count data (after cell and gene filtering) or normalized expression values, based on the description in the method documentation or from the study describing the method. A large portion of the methods requires some form of prior information (for example, a start cell) to be executable. Other methods optionally allow the exploitation of certain prior information. Prior information can be supplied as a starting cell from which the trajectory will originate, a set of important marker genes or even a grouping of cells into cell states. Providing prior information to a TI method can be both a blessing and a curse. In one way, prior information can help the method to find the correct trajectory among many, equally likely, alternatives. On the other hand, incorrect or noisy prior information can bias the trajectory towards current knowledge. Moreover, prior information is not always easily available, and its subjectivity can therefore lead to multiple equally plausible solutions, restricting the applicability of such TI methods to well-studied systems.


The prior information was extracted from the reference trajectory as follows:

\begin{itemize}
	\item \textbf{Start cells}: the identity of one or more start cells. For both real and synthetic data, a cell was chosen that was the closest (in geodesic distance) to each milestone with only outgoing edges. For ties, one random cell was chosen. For cyclic datasets, a random cell was chosen.
	\item \textbf{End cells}: the identity of one or more end cells. This is similar to the start cells, but now for every state with only incoming edges.
	\item \textbf{No. of end states}: number of terminal states, i.e., the number of milestones with only incoming edges.
	\item \textbf{Grouping}: for each cell a label showing which state/cluster/branch it belongs to. For real data, the states were from the gold/silver standard. For synthetic data, each milestone was seen as one group and cells were assigned to their closest milestone.
	\item \textbf{No. of branches}: number of branches/intermediate states. For real data, this was the number of states in the gold/silver standard. For synthetic data, this was the number of milestones.
	\item \textbf{Discrete time course}: for each cell a time point from which it was sampled. If available, this was directly extracted from the reference trajectory; otherwise the geodesic distance from the root milestone was used. For synthetic data, the simulation time was uniformily discretized into four timepoints.
	\item \textbf{Continuous time course}: for each cell a time point from which it was sampled. For real data, this was equal to the discrete time course. For synthetic data, we used the internal simulation time of each simulator.
\end{itemize}



\subsubsection{Common trajectory model}

Due to the absence of a common format for trajectory models, most methods return a unique set of output formats with few overlaps. We therefore post-processed the output of each method into a common probabilistic trajectory model (Figure \ref{fig:supfigure_1}a). This model consisted of three parts. (1) The milestone network represents the overall network topology, and contains edges between different milestones and the length of the edge between them. (2) The milestone percentages contain, for each cell, its position between milestones and sums for each cell to one. (3) The regions of delayed commitment define connections between three or more milestones. These must be explicitly defined in the trajectory model and per region one milestone must be directly connected to all other milestones of the region.

Depending on the output of a method, we used different strategies to convert the output to our model (Figure \ref{fig:supfigure_1}b). Special conversions are denoted by an asterisk and will be explained in more detail in the second list below.



\begin{itemize}
	\item \textbf{Type 1, direct}: CALISTA*, DPT*, ElPiGraph, ElPiGraph cycle, ElPiGraph linear, MERLoT, PAGA, SLICE*, Slingshot, URD* and Wishbone. The wrapped method directly returned a network of milestones, the regions of delayed commitment and for each cell it is given to what extent it belongs to a milestone. In some cases, this indicates that additional transformations were required for the method, not covered by any of the following output formats. Some methods returned a branch network instead of a milestone network and this network was converted by calculating the line graph of the branch network.
	\item \textbf{Type 2, linear pseudotime}: Component 1, Embeddr, FORKS, MATCHER, ouija, ouijaflow, PhenoPath, pseudogp, SCIMITAR, SCORPIUS, topslam, TSCAN, Wanderlust and Waterfall. The method returned a pseudotime, which is translated into a linear trajectory where the milestone network contains two milestones and cells are positioned between these two milestones.
	\item \textbf{Type 3, cyclical pseudotime}: Angle and reCAT. The method returned a pseudotime, which is translated into a cyclical trajectory where the milestone network contains three milestones and cells are positioned between these three milestones. These milestones were positioned at pseudotime 0, 1/3 and 2/3.
	\item \textbf{Type 4, end state probability}: FateID, GPfates, GrandPrix, MFA*, SCOUP and STEMNET. The method returned a pseudotime and for each cell and end state a probability (Pr) for how likely a cell will end up in a certain end state. This was translated into a star-shaped milestone network, with one starting milestone (M0) and several outer milestones (Mi), with regions of delayed commitment between all milestones. The milestone percentage of a cell to one of the outer milestones was equal to pseudotime$\times$PrMi. The milestone percentage to the starting milestone was equal to 1 $-$ pseudotime.
	\item \textbf{Type 5, cluster assignment}: Mpath and SCUBA. The method returned a milestone network and an assignment of each cell to a specific milestone. Cells were positioned onto the milestones they are assigned to, with milestone percentage equal to 1.
	\item \textbf{Type 6, orthogonal projection}: MST, pCreode and RaceID/StemID. The method returned a milestone network, and a dimensionality reduction of the cells and milestones. The cells were projected to the closest nearest segment, thus determining the cells’ position along the milestone network. If a method also returned a cluster assignment (type 5), we limited the projection of each cell to the closest edge connecting to the milestone of a cell. For these methods, we usually wrote two wrappers, one which included the projection and one without.
	\item\textbf{ Type 7, cell graph}: CellRouter, CellTrails, cellTree Gibbs, cellTree maptpx, cellTree VEM, Monocle DDRTree, Monocle ICA, Sincell* and SLICER. The method returned a network of cells and which cell–cell transitions were part of the ‘backbone’ structure. Backbone cells with degree $\neq$ 2 were regarded as milestones and all other cells were placed on transitions between the milestones. If a method did not return a distance between pairs of cells, the cells were uniformly positioned between the two milestones. Otherwise, we first calculated the distance between two milestones as the sum of the distances between the cells and then divided the distance of each pair of cells with the total distance to get the milestone percentages.
\end{itemize}


Special conversions were necessary for certain methods:


\begin{itemize}
	\item \textbf{CALISTA}: We assigned the cells to the branch at which the sum of the cluster probabilities of two connected milestones was the highest. The cluster probabilities of the two selected milestones were then used as milestone percentages. This was then processed as a type 1, direct, method.
	\item \textbf{DPT}: We projected the cells onto the cluster network, consisting of a central milestone (this cluster contained the cells that were assigned to the ‘unknown’ branch) and three terminal milestones, each corresponding to a tip point. This was then processed as a type 1, direct, method.
	\item \textbf{Sincell}: To constrain the number of milestones this method creates, we merged two cell clusters iteratively until the percentage of leaf nodes was below a certain cutoff, with the default cutoff set to 25$\%$. This was then processed as a type 7, cell graph, method.
	\item \textbf{SLICE}: As discussed in the vignette of SLICE (\href{https://research.cchmc.org/pbge/slice.html}{https://research.cchmc.org/pbge/slice.html}), we ran principal curves one by one for every edge detected by SLICE. This was then processed as a type 1, direct, method.
	\item \textbf{MFA}: We used the branch assignment as state probabilities, which together with the global pseudotime were processed as a type 4, end state probabilities, method.
	\item \textbf{URD}: We extracted the pseudotime of a cell within each branch using the y positions in the tree layout. This was then further processed as a type 1, direct, method.
	
\end{itemize}



More information on how each method was wrapped can be found within the comments of each wrapper script, listed at \href{https://methods.dynverse.org}{https://methods.dynverse.org}.

\subsubsection{Off-the-shelf methods}

For baseline performance, we added several ‘off-the-shelf’ TI methods that can be run using a few lines of code in R.

\begin{itemize}
	
	\item \textbf{Component 1}: This method returns the first component of a principal component analysis (PCA) dimensionality reduction as a linear trajectory. This method is especially relevant as it has been used in a few studies already \cite{kouno_temporaldynamicstranscriptional_2013,zeng_pseudotemporalorderingsingle_2017}.
	
	\item \textbf{Angle}: Similar to the previous method, this method computes the angle with respect to the origin in a two-dimensional PCA and uses this angle as a pseudotime for generating a cyclical trajectory.
	
	\item \textbf{MST}: This method performs PCA dimensionality reduction, followed by clustering using the R mclust package, after which the clusters are connected using a minimum spanning tree. The trees are orthogonally projected to the nearest segment of the tree. This baseline is highly relevant as many methods follow the same methodology: dimensionality reduction, clustering, topology inference and project cells to topology.
	
\end{itemize}

\subsection{Trajectory types}

We classified all possible trajectory topologies into distinct trajectory types, based on topological criteria (Figure \ref{fig:figure_1}c). These trajectory types start from the most general trajectory type, a disconnected graph, and move down (within a directed acyclic graph structure), progressively becoming more simple until the two basic types are reached: linear and cyclical. A disconnected graph is a graph in which only one edge can exist between two nodes. A (connected) graph is a disconnected graph in which all nodes are connected. An acyclic graph is a graph containing no cycles. A tree is an acyclic graph containing no convergences (no nodes with in-degree higher than 1). A convergence is an acyclic graph in which only one node has a degree larger than 1 and this same node has an in-degree of 1. A multifurcation is a tree in which only one node has a degree larger than 1. A bifurcation is a multifurcation in which only one node has a degree equal to 3. A linear topology is a graph in which no node has a degree larger than 3. Finally, a cycle is a connected graph in which every node has a degree equal to 2. In most cases, a method that was able to detect a complex trajectory type was also able to detect less complex trajectory types, with some exceptions shown in Figure \ref{fig:figure_2}a.

For simplicity, we merged the bifurcation and convergence trajectory type, and the acyclic graph and connected graph trajectory type in the main figures of the paper.


\subsection{Real datasets}

We gathered real datasets by searching for ‘single-cell’ at the Gene Expression Omnibus and selecting those datasets in which the cells are sampled from different stages in a dynamic process (Supplementary Table 2). The scripts to download and process these datasets are available on our repository (\href{https://benchmark.dynverse.org/tree/master/scripts/01-datasets}{https://benchmark.dynverse.org/tree/master/scripts/01-datasets}). Whenever possible, we preferred to start from the raw counts data. These raw counts were all normalized and filtered using a common pipeline, as discussed later. Some original datasets contained more than one trajectory, in which case we split the dataset into its separate connected trajectory, but also generated several combinations of connected trajectories to include some datasets with disconnected trajectories in the evaluation. In the end, we included 110 datasets for this evaluation.

For each dataset, we extracted a reference trajectory, consisting of two parts: the cellular grouping (milestones) and the connections between these groups (milestone network). The cellular grouping was provided by the authors of the original study, and we classified it as a gold standard when it was created independently from the expression matrix (such as from cell sorting, the origin of the sample, the time it was sampled or cellular mixing) or as a silver standard otherwise (usually by clustering the expression values). To connect these cell groups, we used the original study to determine the network that the authors validated or otherwise found to be the most likely. In the end, each group of cells was placed on a milestone, having a percentage of 1 for that particular milestone. The known connections between these groups were used to construct the milestone network. If there was biological or experimental time data available, we used this as the length of the edge; otherwise we set all the lengths equal to one.

\subsection{Synthetic datasets}

To generate synthetic datasets, we used four different synthetic data simulators:

\begin{itemize}
	\item \textbf{dyngen}: simulations of gene regulatory networks, available at \url{https://github.com/dynverse/dyngen}
	\item \textbf{dyntoy}: random gradients of expression in the reduced space, available at \url{https://github.com/dynverse/dyntoy}
	\item \textbf{PROSSTT}: expression is sampled from a linear model that depends on pseudotime \cite{papadopoulos_prossttprobabilisticsimulation_2018}
	\item \textbf{Splatter}: simulations of non-linear paths between different expression states \cite{zappia_splattersimulationsinglecell_2017}
\end{itemize}


For every simulator, we took great care to make the datasets as realistic as possible. To do this, we extracted several parameters from all real datasets. We calculated the number of differentially expressed features within a trajectory using a two-way Mann–Whitney U test between every pair of cell groups. These values were corrected for multiple testing using the Benjamini-Hochberg procedure (FDR < 0.05) and we required that a gene was expressed in at least 5$\%$ of cells, and had at least a fold-change of 2. We also calculated several other parameters, such as drop-out rates and library sizes using the Splatter package \cite{zappia_splattersimulationsinglecell_2017}. These parameters were then given to the simulators when applicable, as described for each simulator below. Not every real dataset was selected to serve as a reference for a synthetic dataset. Instead, we chose a set of ten distinct reference real datasets by clustering all the parameters of each real dataset, and used the reference real datasets at the cluster centers from a pam clustering (with $k = 10$, implemented in the R cluster package) to generate synthetic data.

\subsubsection{dyngen}

The dyngen (\href{https://github.com/dynverse/dyngen}{https://github.com/dynverse/dyngen}) workflow to generate synthetic data is based on the well established workflow used in the evaluation of network inference methods \cite{schaffter_genenetweaversilicobenchmark_2011,marbach_wisdomcrowdsrobust_2012} and consists of four main steps: network generation, simulation, gold standard extraction and simulation of the scRNA-seq experiment. At every step, we tried to mirror real regulatory networks, while keeping the model simple and easily extendable. We simulated a total of 110 datasets, with 11 different topologies.

\hfill\break
\textit{Network generation}
\hfill\break

One of the main processes involved in cellular dynamic processes is gene regulation, where regulatory cascades and feedback loops lead to progressive changes in expression and decision making. The exact way a cell chooses a certain path during its differentiation is still an active research field, although certain models have already emerged and been tested in vivo. One driver of bifurcation seems to be mutual antagonism, where genes \cite{xu_regulationbifurcatingcell_2015} strongly repress each other, forcing one of the two to become inactive \cite{graf_forcingcellschange_2009}. Such mutual antagonism can be modelled and simulated \cite{wang_quantifyingwaddingtonlandscape_2011,ferrell_bistabilitybifurcationswaddington_2012}. Although such a two-gene model is simple and elegant, the reality is frequently more complex, with multiple genes (grouped into modules) repressing each other \cite{yosef_dynamicregulatorynetwork_2013}.

To simulate certain trajectory topologies, we therefore designed module networks in which the cells follow a particular trajectory topology given certain parameters. Two module networks generated linear trajectories (linear and linear long), one generated a bifurcation, one generated a convergence, one generated a multifurcation (trifurcating), two generated a tree (consecutive bifurcating and binary tree), one generated an acyclic graph (bifurcating and converging), one generated a complex fork (trifurcating), one generated a rooted tree (consecutive bifurcating) and two generated simple graph structures (bifurcating loop and bifurcating cycle). The structure of these module networks is available at \href{https://github.com/dynverse/dyngen/tree/master/inst/ext\_data/modulenetworks}{https://github.com/dynverse/dyngen/tree/master/inst/ext\_data/modulenetworks}.

From these module networks we generated gene regulatory networks in two steps: the main regulatory network was first generated, and extra target genes from real regulatory networks  were added. For each dataset, we used the same number of genes as were differentially expressed in the real datasets. 5$\%$ of the genes were assigned to be part of the main regulatory network, and were randomly distributed among all modules (with at least one gene per module). We sampled edges between these individual genes (according to the module network) using a uniform distribution between 1 and the number of possible targets in each module. To add additional target genes to the network, we assigned every regulator from the network to a real regulator in a real network (from regulatory circuits \cite{marbach_tissuespecificregulatorycircuits_2016}), and extracted for every regulator a local network around it using personalized pagerank (with damping factor set to 0.1), as implemented in the page\_rank function of the \textit{igraph} package. 

\hfill\break
\textit{Simulation of gene regulatory systems using thermodynamic models}
\hfill\break

To simulate the gene regulatory network, we used a system of differential equations similar to those used in evaluations of gene regulatory network inference methods \cite{marbach_wisdomcrowdsrobust_2012}. In this model, the changes in gene expression ($x_i$) and protein expression ($y_i$) are modeled using ordinary differential equations \cite{schaffter_genenetweaversilicobenchmark_2011} (ODEs):



$$
\begin{aligned}
\label{eq:mrna_ode}
\frac{dx_i}{dt} &= \underbrace{m \times f(y_1, y_2, ...)}_\text{production} - \underbrace{\lambda \times x_i}_\text{degradation}
\end{aligned}
$$
$$
\begin{aligned}
\label{eq:prot_ode}
\frac{dy_i}{dt} &= \underbrace{r \times x_i}_\text{production} - \underbrace{\Lambda \times y_i}_\text{degradation}
\end{aligned}
$$

where $m$, $\lambda$, $r$ and $\Lambda$ represent production and degradation rates, the ratio of which determines the maximal gene and protein expression. The two types of equations are coupled because the production of protein $y_i$ depends on the amount of gene expression $x_i$, which in turn depends on the amount of other proteins through the activation function $f(y_1, y_2, ...)$.

The activation function is inspired by a thermodynamic model of gene regulation, in which the promoter of a gene can be bound or unbound by a set of transcription factors, each representing a certain state of the promoter. Each state is linked with a relative activation $\alpha_j$, a number between 0 and 1 representing the activity of the promoter at this particular state. The production rate of the gene is calculated by combining the probabilities of the promoter being in each state with the relative activation:

$$
\begin{aligned}
\label{eq:input_function_probabilities}
f(y_1, y_2, ..., y_n) = \sum_{j \in \{0, 1, ..., n^2\}} \alpha_j \times P_j
\end{aligned}
$$

The probability of being in a state is based on the thermodynamics of transcription factor binding. When only one transcription factor is bound in a state:
$$
\begin{aligned}
P_j \propto \nu = \left(\frac{y}{k}\right)^{n}
\end{aligned}
$$

where the hill coefficient $n$ represents the cooperativity of binding and $k$ the transcription factor concentration at half-maximal binding. When multiple regulators are bound:
$$
\begin{aligned}
P_j \propto \nu =  \rho \times \prod_j \left(\frac{y_j}{k_j}\right)^{n_j}
\end{aligned}
$$

where $\rho$ represents the cooperativity of binding between the different transcription factors. 

$P_i$ is only proportional to $\nu$ because $\nu$ is normalized such that $\sum_{i} P_i = 1$.

To each differential equation, we added an additional stochastic term: 
$$
\begin{aligned}
\frac{dx_i}{dt} & = m \times f(y_1, y_2, ...) - \lambda \times x_i + \eta \times \sqrt{x_i} \times \Delta W_t \\
\frac{dy_i}{dt} & = r \times x_i - \Lambda \times y_i + \eta \times \sqrt{y_i} \times \Delta W_t
\end{aligned}
$$

with $\Delta W_t \sim \mathcal{N}(0, h)$. 

Similar to GeneNetWeaver \cite{schaffter_genenetweaversilicobenchmark_2011}, we sample the different parameters from random distributions, defined as follows. $e$ defines whether a transcription factor activates (1) or represses (-1), as defined within the regulatory network network.

$$
\begin{aligned}
r & = \mathcal{U}(10, 200) \\
d & = \mathcal{U}(2, 8) \\
p & = \mathcal{U}(2, 8) \\
q & = \mathcal{U}(1, 5) \\
a_0 & = \begin{cases}1 & \text{if } |e| = 0 \\ 1 & \text{if } \forall x \in e \text{, } x = -1 \\ 0 & \text{if } \forall x \in e \text{, } x = 1 \\ 0.5 & \text{otherwise}\end{cases} \\
a_i & = \begin{cases}0 & \text{if } \exists x \in e_i \text{, } x = -1 \\ 1 & \text{otherwise}\end{cases} \\
s & = \mathcal{U}(1, 20) \\
k & = y_{max} / (2 * s) \text{,} \\
\ & \quad \text{where } y_{max} = r / d \times p / q \\
c & = \mathcal{U}(1, 4)
\end{aligned}
$$

We converted each ODE to an SDE by adding a chemical Langevin equation, as described in \cite{schaffter_genenetweaversilicobenchmark_2011}. These SDEs were simulated using the Euler–Maruyama approximation, with time-step $h = 0.01$ and noise strength $\eta = 8$. The total simulation time varied between 5 for linear and bifurcating datasets, 10 for consecutive bifurcating, trifurcating and converging datasets, 15 for bifurcating converging datasets and 30 for linear long, cycle and bifurcating loop datasets. The burn-in period was for each simulation 2. Each network was simulated 32 times.

\hfill\break
\textit{Simulation of the single-cell RNA-seq experiment}
\hfill\break

For each dataset we sampled the same number of cells as were present in the reference real dataset, limited to the simulation steps after burn-in. These cells were sampled uniformly across the different steps of the 32 simulations.  Next, we used the Splatter package \cite{zappia_splattersimulationsinglecell_2017} to estimate the different characteristics of a real dataset, such as the distributions of average gene expression, library sizes and dropout probabilities. We used Splatter to simulate the expression levels $\lambda_{i,j}$ of housekeeping genes $i$ (to match the number of genes in the reference dataset) in every cell $j$. These were combined with the expression levels of the genes simulated within a trajectory. Next, true counts were simulated using $Y'_{i,j} \sim \text{Poisson}(\lambda_{i,j})$. Finally, we simulated dropouts by setting true counts to zero by sampling from a Bernoulli distribution using a dropout probability $\pi^D_{i,j} =\frac{1}{1+e^{-k(\text{ln}(\lambda_{i,j})-x_0)}}$. Both $x_0$ (the midpoint for the dropout logistic function) and $k$ (the shape of the dropout logistic function) were estimated by Splatter.

This count matrix was then filtered and normalised using the pipeline described below.

\hfill\break
\textit{Gold standard extraction}
\hfill\break

Because each cellular simulation follows the trajectory at its own speed, knowing the exact position of a cell within the trajectory topology is not straightforward. Furthermore, the speed at  which simulated cells make a decision between two or more alternative paths is highly variable. We therefore first constructed a backbone expression profile for each branch within the trajectory. To do this, we first defined in which order the expression of the modules is expected to change, and then generated a backbone expression profile in which the expression of these modules increases and decreases smoothly between 0 and 1.  We also smoothed the expression in each simulation using a rolling mean with a window of 50 time steps, and then calculated the average module expression along the simulation.  We used dynamic time warping, implemented in the dtw R package \cite{giorgino_computingvisualizingdynamic_2009,tormene_matchingincompletetime_2009}, with an open end to align a simulation to all possible module progressions, and then picked the alignment which minimised the normalised distance between the simulation and the backbone. In case of cyclical trajectory topologies, the number of possible milestones a backbone could progress through was limited to 20.

\subsubsection{dyntoy}

For more simplistic data generation ("toy" datasets), we created the dyntoy workflow (\url{https://github.com/dynverse/dyntoy}). We created 12 topology generators (described below), and with 10 datasets per generator, this lead to a total of 120 datasets.

We created a set of topology generators, were $B(n, p)$ denotes a binomial distribution, and $U(a, b)$ denotes a uniform distribution:

\begin{itemize}
	\item Linear and cyclic, with number of milestones $\sim B(10, 0.25)$
	\item Bifurcating and converging, with four milestones
	\item Binary tree, with number of branching points $\sim U(3, 6)$
	\item Tree, with number of branching points $\sim U(3, 6)$ and maximal degree $\sim U(3, 6)$
\end{itemize} 

For more complex topologies we first calculated a random number of "modifications" $\sim U(3, 6)$ and a $\textit{deg}_{\textit{max}} \sim B(10, 0.25) + 1$. For each type of topology, we defined what kind of modifications are possible: divergences, loops, convergences and divergence-convergence. We then iteratively constructed the topology by uniformly sampling from the set of possible modifications, and adding this modification to the existing topology. For a divergence, we connected an existing milestone to a number of a new milestones. Conversely, for a convergence we connected a number of new nodes to an existing node. For a loop, we connected two existing milestones with a number of milestones in between. Finally for a divergence-convergence we connected an existing milestone to several new milestones which again converged on a new milestone. The number of nodes was sampled from $\sim B(\textit{deg}_{\textit{max}} - 3, 0.25) + 2$

\begin{itemize}
	\item Looping, allowed loop modifications
	\item Diverging-converging, allowed divergence and converging modifications
	\item Diverging with loops, allowed divergence and loop modifications
	\item Multiple looping, allowed looping modifications
	\item Connected, allowed looping, divergence and convergence modifications
	\item Disconnected, number of components sampled from $\sim B(5, 0.25) + 2$, for each component we randomly chose a topology from the ones listed above
\end{itemize}

After generating the topology, we sampled the length of each edge $\sim U(0.5, 1)$. We added regions of delayed commitment to a divergence in a random half of the cases. We then placed the number of cells (same number as from the reference real dataset), on this topology uniformly, based on the length of the edges in the milestone network.

For each gene (same number as from the reference real dataset), we calculated the Kamada-Kawai layout in 2 dimensions, with edge weight equal to the length of the edge. For this gene, we then extracted for each cell a density value using a bivariate normal distribution with $\mu \sim U(x_{\textit{min}}, x_{\textit{min}})$ and $\sigma \sim U(x_{\textit{min}}/10, x_{\textit{min}}/8)$. We used this density as input for a zero-inflated negative binomial distribution with $\mu ~ U(100, 1000) \times \textit{density}$, $k ~ U(\mu / 10, \mu / 4)$ and $pi$ from the parameters of the reference real dataset, to get the final count values.

This count matrix was then filtered and normalised using the pipeline described below.

\subsubsection{PROSSTT}

PROSSTT is a recent data simulator \cite{papadopoulos_prossttprobabilisticsimulation_2018}, which simulates expression using linear mixtures of expression programs and random walks through the trajectory. We used 5 topology generators from dyntoy (linear, bifurcating, multifurcating, binary tree and tree), and simulated for each topology generator 10 datasets using different reference real datasets. However, due to frequent crashes of the tool, only 19 datasets created output and were thus used in the evaluation.

Using the simulate\_lineage function, we simulated the lineage expression, with parameters $a \sim U(0.01, 0.1)$, $\textit{branch-tol}_{\textit{intra}} \sim U(0, 0.9)$ and $\textit{branch-tol}_{\textit{inter}} \sim U(0, 0.9)$. These parameter distributions were chosen very broad so as to make sure both easy and difficult datasets are simulated. After simulating base gene expression with simulate\_base\_gene\_exp, we used the sample\_density function to finally simulate expression values of a number of cells (the same as from the reference real dataset), with $\alpha \sim \textit{Lognormal}$ ($\mu = 0.3$ and $\sigma = 1.5$) and $\beta \sim \textit{Lognormal}$ ($\mu = 2$ and $\sigma = 1.5$). Each of these parameters were centered around the default values of PROSSTT, but with enough variability to ensure a varied set of datasets.

This count matrix was then filtered and normalised using the pipeline described below.

\subsubsection{Splatter}

Splatter \cite{zappia_splattersimulationsinglecell_2017} simulates expression values by constructing non-linear paths between different states, each having a distinct expression profile. We used 5 topology generators from dyntoy (linear, bifurcating, multifurcating, binary tree and tree), and simulated for each topology generator 10 datasets using different reference real datasets, leading to a total of 50 datasets.

We used the splatSimulatePaths function from Splatter to simulate datasets, with number of cells and genes equal to those in the reference real dataset, and with parameters  $\textit{nonlinearProb}$, $\textit{sigmaFac}$ and $\textit{skew}$ all sampled from $U(0, 1)$.

\subsection{Dataset filtering and normalization}

We used a standard single-cell RNA-seq preprocessing pipeline that applies parts of the scran and scater Bioconductor packages \cite{lun_stepbystepworkflowlowlevel_2016}. The advantages of this pipeline are that it works both with and without spike-ins, and it includes a harsh cell filtering that looks at abnormalities in library sizes, mitochondrial gene expression and the number of genes expressed using median absolute deviations (which we set to 3). We required that a gene was expressed in at least 5$\%$ of the cells and that it should have an average expression higher than 0.02. Furthermore, we used the pipeline to select the most highly variable genes, using a false discovery rate of 5$\%$ and a biological component higher than 0.5. As a final filter, we removed both all-zero genes and cells until convergence.

\subsection{Benchmark metrics}

The importance of using multiple metrics to compare complex models has been stated repeatedly \cite{norel_selfassessmenttrapcan_2011}. Furthermore, a trajectory is a model with multiple layers of complexity, which calls for several metrics each assessing a different layer. We therefore defined several possible metrics for comparing trajectories, each investigating different layers. These are all discussed in Supplementary Note 1 along with examples and robustness analyses when appropriate.

Next, we created a set of rules to which we think a good trajectory metric should conform, and tested this empirically for each metric by comparing scores before and after perturbing a dataset (Supplementary Note 1). Based on this analysis, we chose four metrics for the evaluation, each assessing a different aspect of the trajectory: (1) the HIM measures the topological similarity; (2) the F1branches compares the branch assignment; (3) the cordist assesses the similarity in pairwise cell–cell distances and thus the cellular positions; and (4) the wcorfeatures looks at whether similar important features (genes) are found in both the reference dataset and the prediction.

\subsubsection{The Hamming–Ipsen–Mikhailov metric}

The HIM metric \cite{jurman_himglocalmetric_2015} uses the two weighted adjacency matrices of the milestone networks as input (weighted by edge length). It is a linear combination of the normalized Hamming distance, which gives an indication of the differences in edge lengths, and the normalized Ipsen–Mikhailov distance, which assesses the similarity in degree distributions. The latter has a parameter $\gamma$, which was fixed at 0.1 to make the scores comparable between datasets. We illustrate the metric and discuss alternatives in Supplementary Note 1.

\subsubsection{The F1 between branch assignments}

To compare branch assignment, we used an F1 score, also used used for comparing biclustering methods \cite{saelens_comprehensiveevaluationmodule_2018}. To calculate this metric, we first calculated the similarity of all pairs of branches between the two trajectories using the Jaccard similarity. Next, we defined the ‘Recovery' (respectively ‘Relevance') as the average maximal similarity of all branches in the reference dataset (respectively prediction). The F1branches was then defined as the harmonic mean between Recovery and Relevance. We illustrate this metric further in Supplementary Note 1.

\subsubsection{Correlation between geodesic distances}

When the position of a cell is the same in both the reference and the prediction, its relative distances to all other cells in the trajectory should also be the same. This observation is the basis for the cordist metric. To calculate the cordist, we first sampled 100 waypoint cells in both the prediction and the reference dataset, using stratified sampling between the different milestones, edges and regions of delayed commitment, weighted by the number of cells in each collection. We then calculated the geodesic distances between the union of waypoint cells from both datasets and all other cells. The calculation of the geodesic distance depended on the location of the two cells within the trajectory, further discussed in Supplementary Note 1, and was weighted by the length of the edge in the milestone network. Finally, the cordist was defined as the Spearman rank correlation between the distances of both datasets. We illustrate the metric and assess the effect of the number of waypoint cells in Supplementary Note 1.

\subsubsection{The correlation between important features}

The wcorfeatures assesses whether the same differentially expressed features are found using the predicted trajectory as in the known trajectory. To calculate this metric, we used Random Forest regression (implemented in the R ranger package \cite{wright_rangerfastimplementation_2017}), to predict expression values of each gene, based on the geodesic distances of a cell to each milestone. We then extracted feature importance values for each feature and calculated the similarity of the feature importances using a weighted Pearson correlation, weighted by the feature importance in the reference dataset to give more weight to large differences. As hyperparameters we set the number of trees to 10,000 and the number of features on which to split to 1$\%$ of all available features. We illustrate this metric and assess the effect of its hyperparameters in Supplementary Note 1.

\subsubsection{Score aggregation}

To rank methods, we needed to aggregate the different scores on two levels: across datasets and across different metrics. This aggregation strategy is explained in more detail in Supplementary Note 1.

To ensure that easy and difficult datasets have equal influence on the final score, we first normalized the scores on each dataset across the different methods. We shifted and scaled the scores to $\sigma$ = 1 and $\mu$ = 0, and then applied the unit probability density function of a normal distribution on these values to get the scores back into the [0,1] range.

Since there is a bias in dataset source and trajectory type (for example, there are many more linear datasets), we aggregated the scores per method and dataset in multiple steps. We first aggregated the datasets with the same dataset source and trajectory type using an arithmetic mean of their scores. Next, the scores were averaged over different dataset sources, using an arithmetic mean that was weighted based on how much the synthetic and silver scores correlated with the real gold scores. Finally, the scores were aggregated over the different trajectory types again using an arithmetic mean.

Finally, to get an overall benchmarking score, we aggregated the different metrics using a geometric mean.

\subsection{Method execution}

Each execution of a method on a dataset was performed in a separate task as part of a gridengine job. Each task was allocated one CPU core of an Intel(R) Xeon(R) CPU E5-2665 at 2.40 GHz, and one R session was started for each task. During the execution of a method on a dataset, if the time limit (>1 h) or memory limit (16 GB) was exceeded, or an error was produced, a zero score was returned for that execution.

\subsection{Complementarity}

To assess the complementarity between different methods, we first calculated for every method and dataset whether the overall score was equal to or higher than 95$\%$ of the best overall score for that particular dataset. We then calculated for every method the weighted percentage of datasets that fulfilled this rule, weighted similarly as in the benchmark aggregation, and chose the best method. We iteratively added new methods until all methods were selected. For this analysis, we did not include any methods that require any strong prior information and only included methods that could detect the trajectory types present in at least one of the datasets.

\subsection{Scalability}

To assess the scalability of each method, we started from five real datasets, selected using the centers from a k-medoids as discussed before. We up- and downscaled these datasets between 10 and 100,000 cells and 10 and 100,000 features, while never going higher than 1,000,000 values in total. To generate new cells or features, we first generated a 10-nearest-neighbor graph of both the cells and features from the expression space. For every new cell or feature, we used a linear combination of one to three existing cells or features, where each cell or feature was given a weight sampled from a uniform distribution between 0 and 1.

We ran each method on each dataset for maximally 1 h and gave each process 10 GB of memory. To determine the running time of each method, we started the timer right after data loading and the loading of any packages, and stopped the clock before postprocessing and saving of the output. Pre- and postprocessing steps specific to a method, such as dimensionality reduction and gene filtering, were included in the time. To estimate the maximal memory usage, we used the max\_vmem value from the qacct command provided by a gridengine cluster. We acknowledge, however, that these memory estimates are very noisy and the averages provided in this study are therefore only rough estimates.

The relationship between the dimensions of a dataset and the running time or maximal memory usage was modeled using shape constrained additive models \cite{pya_shapeconstrainedadditive_2015}, with $log_{10}|\textit{cells}|$ and $log_{10}|\textit{features}|$ as predictor variables, and fitted this model using the scam function as implemented in the R scam package, with $log_{10}\textit{time}$ (or $log_{10}\textit{memory}$) as outcome.

To classify the time complexity of each method with respect to the number of cells, we predicted the running time at 10,000 features with increasing number of cells from 100 to 100,000, with steps of 100. We trained a generalized linear model with the following function: $y \approxeq \log{x} + \sqrt{x} + x + x^2 + x^3$ with y as running time and x as the number of cells or features. The time complexity of a method was then classified using the weights w from this model:

$$
\begin{cases}
\text{superquadratic} & \text{if } w_{x^3} > 0.25,\\
\text{quadratic} & \text{if } w_{x^2} > 0.25,\\
\text{linear} & \text{if } w_x > 0.25,\\
\text{sublinear} & \text{if } w_{\text{log}(x)} > 0.25 \text{ or } w_{\text{sqrt}(x)} > 0.25,\\
\text{case with highest weight} & \text{else.}
\end{cases}
$$

This process was repeated for classifying the time complexity with respect to the number of features, and the memory complexity both with respect to the number of cells and features.

\subsection{Stability}

In the ideal case, a method should produce a similar trajectory, even when the input data is slightly different. However, running the method multiple times on the same input data would not be the ideal approach to assess its stability, given that a lot of tools are artificially deterministic by internally resetting the pseudorandom number generator (for example, using the `set.seed` function in R or the `random.seed` function in numpy). To assess the stability of each method, we therefore selected a number of datasets, which consisted of 25$\%$ of the datasets accounting for 15$\%$ of the total runtime, chosen such that after aggregation the overall scores still has > 0.99 correlation with the original overall ranking. We subsampled each dataset 10 times with 95$\%$ of the original cells and 95$\%$ of the original features. We ran every method on each of the bootstraps, and assessed the stability by calculating the benchmarking scores between each pair of subsequent models (run $i$ is compared to run $i+1$). For the cordist and F1branches, we only used the intersection between the cells of two datasets, while the intersection of the features was used for the wcorfeatures.

\subsection{Usability}

We created a transparent scoring scheme to quantify the usability of each method based on several existing tool quality and programming guidelines in the literature and online (Table \ref{tab:scoresheet}). The main goal of this quality control is to stimulate the improvement of current methods, and the development of user- and developer-friendly new methods. The quality control assessed six categories, each looking at several aspects, which are further divided into individual items. The availability category checks whether the method is easily available, whether the code and dependencies can be easily installed, and how the method can be used. The code quality assesses the quality of the code both from a user perspective (function naming, dummy proofing and availability of plotting functions) and a developer perspective (consistent style and code duplication). The code assurance category is frequently overlooked, and checks for code testing, continuous integration \cite{beaulieu-jones_reproducibilitycomputationalworkflows_2017} and an active support system. The documentation category checks the quality of the documentation, both externally (tutorials and function documentation) and internally (inline documentation). The behavior category assesses the ease by which the method can be run, by looking for unexpected output files and messages, prior information and how easy the trajectory model can be extracted from the output. Finally, we also assessed certain aspects of the study in which the method was proposed, such as publication in a peer-reviewed journal, the number of datasets in which the usefulness of the method was shown and the scope of method evaluation in the paper.

Each quality aspect received a weight depending on how frequently it was found in several papers and online sources that discuss tool quality (Table \ref{tab:scoresheet}). This was to make sure that more important aspects, such as the open source availability of the method, outweighed other less important aspects, such as the availability of a graphical user interface. For each aspect, we also assigned a weight to the individual questions being investigated (Table \ref{tab:scoresheet}). For calculating the final score, we weighed each of the six categories equally.

\subsection{Guidelines}

For each set of outcomes in the guidelines figure, we selected one to four methods, by first filtering the methods on those that can detect all required trajectory types, and ordering the methods according to their average accuracy score on datasets containing these trajectory types (aggregated according to the scheme presented in the section Accuracy).

We used the same approach for selecting the best set of methods in the guidelines app (\href{http://guidelines.dynverse.org}{http://guidelines.dynverse.org}), developed using the R shiny package. This app will also filter the methods, among other things, depending on the predicted running time and memory requirements, the prior information available and the preferred execution environment (using the dynmethods package or standalone).

\subsection{Reporting Summary}

Further information on research design is available in the Nature Research Reporting Summary, available at \href{https://www.nature.com/articles/s41587-019-0071-9\#MOESM2}{https://www.nature.com/articles/s41587-019-0071-9\#MOESM2}

\section{Supplementary Note 1: Metrics to compare two trajectories}

A trajectory, as defined in our evaluation, is a model with multiple abstractions. The top abstraction is the topology which contains information about the paths each cell can take from their starting point. Deeper abstractions involve the mapping of each cell to a particular branch within this network, and the position (or ordering) of each cells within these branches. Internally, the topology is represented by the milestone network and regions of delayed commitment, the branch assignment and cellular positions are represented by the milestone percentages (Figure \ref{fig:snote1fig_1}).

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_1.pdf}
	\caption{
		\textbf{An example trajectory that will be used throughout this section.}
		It contains contains four milestones (W to Z) and five cells (a to e).
	}
	\label{fig:snote1fig_1}
\end{figure}

Given the multilayered complexity of a trajectory model, it is not trivial to compare the similarity of two trajectory models using only one metric. We therefore sought to use different comparison metrics, each serving a different purpose:

\begin{itemize}
	\item \textbf{Specific metrics} investigate one particular aspect of the trajectory. Such metrics make it possible to find particular weak points for methods, e.g. that a method is very good at ordering but does not frequently find the correct topology. Moreover, having multiple individual metrics allow personalised rankings of methods, for example for users which are primarily interested in using the method correct topology.
	\item \textbf{Application metrics} focus on the quality of a downstream analysis using the trajectory. For example, it measures whether the trajectory can be used to find accurate differentially expressed genes.
	\item \textbf{Overall metrics} should capture all the different abstractions, in other words such metrics measure whether the resulting trajectory has a good topology, that the cells belong to similar branches \textit{and} that they are ordered correctly.
\end{itemize}

Here, we first describe and illustrate several possible specific, application and overall metrics. Next, we test these metrics on several test cases, to make sure they robustly identify "wrong" trajectory predictions.

All metrics described here were implemented within the dyneval R package (\url{https://github.com/dynverse/dyneval}).

\subsection{Metric characterisation and testing}

\subsubsection{\textit{isomorphic}, \textit{edgeflip} and \textit{HIM}: Edit distance between two trajectory topologies}

We used three different scores to assess the similarity in the topology between two trajectories, irregardless of where the cells were positioned.

For all three scores, we first simplified the topology of the trajectory to make both graph structures comparable:

\begin{itemize}
	\item As we are only interested in the main structure of the topology without start or end, the graph was made undirected.
	\item All milestones with degree 2 were removed. For example in the topology A $\Rightarrow$ B $\Rightarrow$ C $\Rightarrow$ D, C $\Rightarrow$ D, the B milestone was removed
	\item A linear topology was converted to A $\Rightarrow$ B $\Rightarrow$ C
	\item A cyclical topology such as A $\Rightarrow$ B $\Rightarrow$ C $\Rightarrow$ D or A $\Rightarrow$ B $\Rightarrow$ A were all simplified to A $\Rightarrow$ B $\Rightarrow$ C $\Rightarrow$ A
	\item Duplicated edges such as A $\Rightarrow$ B, A $\Rightarrow$ B were decoupled to A $\Rightarrow$ B, A $\Rightarrow$ C $\Rightarrow$ B
\end{itemize}

The \textit{isomorphic} score returns 1 if two graphs are isomorphic, and 0 if they were not. For this, we used the used the BLISS algorithm \cite{junttila_engineeringefficientcanonical_2007}, as implemented in the R *igraph* package.

The \textit{edgeflip} score was defined as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem without a scalable solution \cite{bahiense_maximumcommonedge_2012}. We implemented a branch and bound approach for this problem, using several heuristics to speed up the search:

\begin{itemize}
	\item First check all possible edge additions and removals corresponding to the number of different edges between the two graphs.
	\item For each possible solution, first check whether: \begin{enumerate}
		\item The maximal degree is the same
		\item The minimal degree is the same
		\item All degrees are the same after sorting
	\end{enumerate}
	\item Only then check if the two graphs are isomorphic as described earlier.
	\item If no solution is found, check all possible solutions with two extra edge additions/removals.
\end{itemize}

The \textit{HIM} metric (Hamming-Ipsen-Mikhailov distance) \cite{jurman_himglocalmetric_2015} which was adopted from the R nettools package (\href{https://github.com/filosi/nettools}{https://github.com/filosi/nettools}). It uses an adjacency matrix which was weighted according to the lengths of each edges within the milestone network. Conceptually, \textit{HIM} is a linear combination of:

\begin{itemize}
	\item The normalised Hamming distance \cite{dougherty_validationgeneregulatory_2011}, which calculates the distance between two graphs by matching individual edges in the adjacency matrix, but disregards overall structural similarity.
	\item The normalised Ipsen-Mikhailov distance \cite{ipsen_evolutionaryreconstructionnetworks_2002}, which calculates the overall distance of two graphs based on matches between its degree and adjacency matrix, while disregarding local structural similarities. It requires a $\gamma$ parameter, which is usually estimated based on the number of nodes in the graph, but which we fixed at $0.1$ so as to make the score comparable across different graph sizes.
\end{itemize}

We compared the three scores on several common topologies (Figure \ref{fig:snote1fig_2}a). While conceptually very different, the \textit{edgeflip} and \textit{HIM} still produce similar scores (Figure \ref{fig:snote1fig_2}b). The \textit{HIM} tends to punish the detection of cycles, while the \textit{edgeflip} is more harsh for differences in the number of bifurcations (Figure \ref{fig:snote1fig_2}b). The main difference however is that the \textit{HIM} takes into account edge lengths when comparing two trajectories, as illustrated in (Figure \ref{fig:snote1fig_2}c). Short "extra" edges in the topology are less punished by the \textit{HIM} than by the \textit{edgeflip}.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_2.pdf}
	\caption{
		\textbf{Showcase of three metrics to evaluate topologies: \textit{isomorphic}, \textit{edgeflip} and \textit{HIM}}
		\textbf{(a)} The used topologies. \textbf{(b)} The scores when comparing each pair of trajectory types. (c) Four datasets in which aan extra edge is added and made progressively longer. This shows how the HIM can take into account edge lengths.
	}
	\label{fig:snote1fig_2}
\end{figure}

To summarise, the different topology based scores are useful for different scenarios:

\begin{itemize}
	\item If the two trajectories should only be compared when the topology is exactly the same, the \textit{isomorphic} should be used.
	\item If it is important that the topologies are similar, but not necessarily isomorphic, the \textit{edgeflip} is most appropriate.
	\item If the topologies should be similar, but shorter edges should not be punished as hard as longer edges, the \textit{HIM} is most appropriate.
\end{itemize}

\subsubsection{$\textit{F1}_{\textit{branches}}$ and $\textit{F1}_{\textit{milestones}}$: Comparing how well the cells are clustered in the trajectory}

Perhaps one of the simplest ways to calculate the similarity between the cellular positions of two topologies is by mapping each cell to its closest milestone \textit{or} branch \ref{fig:snote1fig_3}. These clusters of cells can then be compared using one of the many external cluster evaluation measures \cite{saelens_comprehensiveevaluationmodule_2018}. When selecting a cluster evaluation metric, we had two main conditions:

- Because we allow methods to filter cells in the trajectory, the metric should be able to handle "non-exhaustive assignment", where some cells are not assigned to any cluster.
- The metric should give each cluster equal weight, so that rare cell stages are equally important as large stages.

The $\textit{F1}$ score between the $\textit{Recovery}$ and $\textit{Relevance}$ is a metric which conforms to both these conditions. This metric will map two clustersets by using their shared members based on the $\textit{Jaccard}$ similarity. It then calculates the $\textit{Recovery}$ as the average maximal $\textit{Jaccard}$ for every cluster in the first set of clusters (in our case the reference trajectory). Conversely, the $\textit{Relevance}$ is calculated based on the average maximal similarity in the second set of clusters (in our case the prediction). Both the $\textit{Recovery}$ and $\textit{Relevance}$ are then given equal weight in a harmonic mean ($\textit{F1}$). Formally, if $C$ and $C'$ are two cell clusters:

\begin{align*}
\textit{Jaccard}(c, c') &= \frac{|c \cap c'|}{|c \cup c'|} \\
\textit{Recovery} &= \frac{1}{|C|} \sum_{c \in C}{\max_{c' \in C'}{\textit{Jaccard(c, c')}}} \\
\textit{Relevance} &= \frac{1}{|C'|} \sum_{c' \in C'}{\max_{c \in C}{\textit{Jaccard(c, c')}}} \\
\textit{F1} &= \frac{2}{\frac{1}{\textit{Recovery}} + \frac{1}{\textit{Relevance}}}
\end{align*}

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_3.pdf}
	\caption{
		\textbf{Mapping cells to their closest milestone or branch for the calculation of the {$\textit{F1}_{\textit{milestones}}$} and {$\textit{F1}_{\textit{branches}}$} .}
		To calculate the {$\textit{F1}_{\textit{milestones}}$}, cells are mapped towards the nearest milestone, i.e. the milestone with the highest milestone percentage. For the {$\textit{F1}_{\textit{branches}}$}, the cells are mapped to the closest edge.
	}
	\label{fig:snote1fig_3}
\end{figure}

\subsubsection{$\textit{cor}_{\textit{dist}}$: Correlation between geodesic distances}

When the position of a cell is the same in both the reference and the prediction, its \textit{relative} distances to all other cells in the trajectory should also be the same. This observation is the basis for the $\textit{cor}_{\textit{dist}}$ metric.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_4.pdf}
	\caption{
		\textbf{The calculation of geodesic distances on a small example trajectory.}
		\textbf{a)} A toy example containing four milestones (W to Z) and five cells (a to e). \textbf{b)} The corresponding milestone network, milestone percentages and regions of delayed commitment, when the toy trajectory is converted to the common trajectory model. \textbf{c)} The calculations made for calculating the pairwise geodesic distances. \textbf{d)} A heatmap representation of the pairwise geodesic distances.
	}
	\label{fig:snote1fig_4}
\end{figure}

The geodesic distance is the distance a cell has to go through the trajectory space to get from one position to another. The way this distance is calculated depends on how two cells are positioned, showcased by an example in Figure \ref{fig:snote1fig_4}:

\begin{itemize}
	\item \textbf{Both cells are on the same edge in the milestone network.} In this case, the geodesic distance is defined as the product of the difference in milestone percentages and the length of their shared edge. For cells $a$ and $b$ in the example, $d(a, b)$ is equal to $1 \times (0.9 - 0.2) = 0.7$.
	\item \textbf{Cells reside on different edges in the milestone network.} First, the distance of the cell to all its nearby milestones is calculated, based on its percentage within the edge and the length of the edge. These distances in combination with the milestone network are used to calculate the shortest path distance between the two cells. For cells $a$ and $c$ in the example, $d(a, X) = 1 \times 0.9$ and $d(c, X) = 3 \times 0.2$, and therefore $d(a, c) = 1 \times 0.9 + 3 \times 0.2$. 
\end{itemize}

The geodesic distance can be easily extended towards cells within regions of delayed commitment. When both cells are part of the same region of delayed commitment, the geodesic distance was defined as the manhattan distances between the milestone percentages weighted by the lengths from the milestone network. For cells $d$ and $e$ in the example, $d(d, e)$ is equal to $0 \times (0.3 - 0.2) + 2 \times (0.7 - 0.2) + 3 \times(0.4 - 0.1) = 1.9$. The distance between two cells where only one is part of a region of delayed commitment is calculated similarly to the previous paragraph, by first calculating the distance between the cells and their neighbouring milestones first, then calculating the shortest path distances between the two.

Calculating the pairwise distances between cells scales quadratically with the number of cells, and would therefore not be scaleable for large datasets. For this reason, a set of waypoint cells are defined \textit{a priori}, and only the distances between the waypoint cells and all other cells is calculated, in order to calculate the correlation of geodesic distances of two trajectories (Figure \ref{fig:snote1fig_5}a). These cell waypoints are determined by viewing each milestone, edge and region of delayed commitment as a collection of cells. We do stratified sampling from each collection of cells by weighing them by the total number of cells within that collection. For calculating the $\textit{cor}_{\textit{dist}}$ between two trajectories, the distances between all cells and the union of both waypoint sets is computed.

To select the number of cell waypoints, we need to find a trade-off between the accuracy versus the time to calculate $\textit{cor}_{\textit{dist}}$. To select an optimal number of cell waypoints, we used the synthetic dataset with the most complex topology, and determined the $\textit{cor}_{\textit{dist}}$ at different levels of both cell shuffling and number of cell waypoints (Figure \ref{fig:snote1fig_5}a). We found that using cell waypoints does not induce a systematic bias in the $\textit{cor}_{\textit{dist}}$, and that its variability was relatively minimal when compared to the variability between different levels of cell shuffling when using 100 or more cell waypoints.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_5.pdf}
	\caption{
		\textbf{Determination of cell waypoints} \textbf{a)} Illustration of the stratified cell sampling using an example dataset (top). Each milestone, edge between two milestones and region of delayed commitment is seen as a collection of cells (middle), and the number of waypoints (100 in this case) are divided over each of these collection of cells (bottom). \textbf{b)} Accuracy versus time to calculate $\textit{cor}_{\textit{dist}}$. Shown are distributions over 100 random waypoint samples. The upper whisker of the boxplot extends from the hinge (75$\%$ percentile) to the largest value, no further than 1.5$\times$ the IQR of the hinge. The lower whisker extends from the hinge (25$\%$ percentile) to the smallest value, at most 1.5$\times$ the IQR of the hinge.
	}
	\label{fig:snote1fig_5}
\end{figure}

Although the $\textit{cor}_{\textit{dist}}$'s main characteristic is that it looks at the positions of the cells, other features of the trajectory are also (partly) captured. To illustrate this, we used the geodesic distances themselves as input for dimensionality reduction (Figure \ref{fig:snote1fig_6}) with varying topologies. This reduced space captures the original trajectory structure quite well, including the overall topology and branch lengths.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_6.pdf}
	\caption{
		\textbf{Determination of cell waypoints.} 
		We generated different toy trajectory datasets with varying topologies and calculated the geodesic distances between all cells within the trajectory. We then used these distances as input for classical multidimensional scaling. This shows that the geodesic distances do not only contain information regarding the cell's positions, but also information on the lengths and wiring of the topology.
	}
	\label{fig:snote1fig_6}
\end{figure}

\subsubsection{$\textit{NMSE}_{\textit{rf}}$ and $\textit{NMSE}_{\textit{lm}}$: Using the positions of the cells within one trajectory to predict the cellular positions in the other trajectory}

An alternative approach to detect whether the positions of cells are similar between two trajectories, is to use the positions of one trajectory to predict the positions within the other trajectory. If the cells are at similar positions in the trajectory (relative to its nearby cells), the prediction error should be low.

Specifically, we implemented two metrics which predict the milestone percentages from the reference by using the predicted milestone percentages as features (Figure \ref{fig:snote1fig_7}). We did this with two regression methods, linear regression ($\textit{lm}$, using the R lm function) and Random Forest ($\textit{rf}$, implemented in the \textit{ranger} package  \cite{wright_rangerfastimplementation_2017}). In both cases, the accuracy of the prediction was measured using the Mean Squared error ($\mathit{MSE}$), in the case of Random forest we used the out-of-bag mean-squared error. Next, we calculated $\mathit{MSE}_{worst}$ equal to the $\mathit{MSE}$ when predicting all milestone percentages as the average. We used this to calculate the normalised mean squared error as $\mathit{NMSE} = 1 - \frac{\mathit{MSE}}{\mathit{MSE}_{worst}}$. We created a regression model for every milestone in the gold standard, and averaged the $\mathit{NMSE}$ values to finally obtain the $\textit{NMSE}_{\textit{rf}}$ and $\textit{NMSE}_{\textit{lm}}$ scores.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_7.pdf}
	\caption{
		\textbf{The calculation of $\textit{NMSE}_{\textit{lm}}$ distances on a small example trajectory.} 
		The milestone percentages of the reference are predicted based on the milestone percentages of the prediction, using regression models such as linear regression or random forests. The predicted trajectory is then scored by comparing the mean-squared error (MSE) of this regression model with the baseline MSE where the prediction is the average milestone percentage.
	}
	\label{fig:snote1fig_7}
\end{figure}

\subsubsection{$\textit{cor}_{\textit{features}}$ and $\textit{wcor}_{\textit{features}}$: The accuracy of dynamical differentially expressed features/genes.}

Although most metrics described above already assess some aspects directly relevant to the user, such as whether the method is good at finding the right topology, these metrics do not assess the quality of downstream analyses and hypotheses which can be generated from these models. 

Perhaps the main advantage of studying cellular dynamic processes using single-cell -omics data is that the dynamics of gene expression can be studied for the whole transcriptome. This can be used to construct other models such as dynamic regulatory networks and gene expression modules. Such analyses rely on a "good-enough" cellular ordering, so that it can be used to identify dynamical differentially expressed genes.

To calculate the $\textit{cor}_{\textit{features}}$ we used Random forest regression to rank all the features according to their importance in predicting the positions of cells in the trajectory. More specifically, we first calculated the geodesic distances for each cell to all milestones in the trajectory. Next, we trained a Random Forest regression model (implemented in the R \textit{ranger} package \cite{wright_rangerfastimplementation_2017}, \href{https://github.com/imbs-hl/ranger}{https://github.com/imbs-hl/ranger}) to predict these distances for each milestone, based on the expression of genes within each cell. We then extracted feature importances using the Mean Decrease in Impurity (importance = 'impurity' parameter of the ranger function), as illustrated in Figure \ref{fig:snote1fig_8}. The overall importance of a feature (gene) was then equal to the mean importance over all milestones. Finally, we compared the two rankings by calculating the Pearson correlation, with values between -1 and 0 clipped to 0.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_8.pdf}
	\caption{
		\textbf{An illustration of ranking features based on their importance in a trajectory.} 
		(a) A MDS dimensionality reudction of a real dataset in which mouse embryonic fibroblasts (MEF) differentiate into Neurons and Myocytes. (b) The ranking of feature importances from high to low. The majority of features have a very low importance. (c) Some examples, which were also highlighted in b. Higher features in the ranking are clearly specific to certain parts of the trajectory, while features lower on the ranking have a more dispersed expression pattern.
	}
	\label{fig:snote1fig_8}
\end{figure}

Random forest regression has two main hyperparameters. The number of trees to be fitted (num\_tree parameter) was fixed to 10000 to provide accurate and stable estimates of the feature importance (Figure \ref{fig:snote1fig_9}. The number of features on which can be split (mtry parameter) was set to 1$\%$ of all available features (instead of the default square-root of the number of features), as to make sure that predictive but highly correlated features, omnipresent in transcriptomics data, are not suppressed in the ranking.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_9.pdf}
	\caption{
		\textbf{Effect of the number of trees parameter on the accuracy and variability of the $\textit{cor}_{\textit{features}}$.} 
		We used the dataset from Figure \ref{fig:snote1fig_8} and calculated the $\textit{cor}_{\textit{features}}$ after shuffling a percentage of cells.
	}
	\label{fig:snote1fig_9}
\end{figure}

For most datasets, only a limited number of features will be differentially expressed in the trajectory. For example, in the dataset used in Figure \ref{fig:snote1fig_9} only the top 10$\%$-20$\%$ show a clear pattern of differential expression. The correlation will weight each of these features equally, and will therefore give more weight to the bottom, irrelevant features. To prioritise the top differentially expressed features, we also implemented the $\textit{wcor}_{\textit{features}}$, which will weight the correlation using the feature importance scores in the reference so that the top features have relatively more impact on the score (Figure \ref{fig:snote1fig_10}).

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_10.pdf}
	\caption{
		\textbf{Effect of weighting the features based on their feature importance in the reference.} 
		We used the same dataset as in Figure \ref{fig:snote1fig_8}, and calculated the $\textit{cor}_{\textit{features}}$ after shuffling a percentage of cells.
	}
	\label{fig:snote1fig_10}
\end{figure}

% TODO: check floatbarrier?
%\FloatBarrier

\subsection{Metric conformity}

Although most metrics described in the previous section make sense intuitively, this does not necessarily mean that these metrics are robust and will generate reasonable results when used for benchmarking. This is because different methods and datasets will all lead to a varied set of trajectory models:

\begin{itemize}
	\item Real datasets have all cells grouped onto milestones
	\item Some methods place all cells in a region of delayed commitment, others never generate a region of delayed commitment
	\item Some methods always return a linear trajectory, even if a bifurcation is present in the data
	\item Some methods filter cells
\end{itemize}

A good metric, especially a good overall metric, should work in all these circumstances. To test this, we designed a set of rules to which a good metric should conform, and assessed empirically whether a metric conforms to these rules.

We generated a panel of toy datasets (using our \href{https://github.com/dynverse/dyntoy}{\textit{dyntoy}} package, \href{https://github.com/dynverse/dyntoy}{https://github.com/dynverse/dyntoy}) with all possible combinations of:

\begin{itemize}
	\item \# cells: 10, 20, 50, 100, 200, 500
	\item \# features: 200
	\item topologies: linear, bifurcation, multifurcating, tree, cycle, connected graph and disconnected graph
	\item Whether cells are placed on the milestones (as in real data) or on the edges/regions of delayed commitment between the milestones (as in synthetic data)
\end{itemize}

We then perturbed the trajectories in these datasets in certain ways, and tested whether the scores follow an expected pattern. An overview of the conformity of every metric is first given in Table \ref{tab:conformity_overview}. The individual rules and metric behaviour are discussed in the Supplementary Material that can be found at \href{https://www.nature.com/articles/s41587-019-0071-9\#Sec34}{https://www.nature.com/articles/s41587-019-0071-9\#Sec34}.

\begin{table}
	\caption{Overview of whether a particular metric conforms to a particular rule} \label{tab:conformity_overview}
	
	\centering\begingroup\fontsize{7}{9}\selectfont
	
	\begin{tabular}{>{\raggedright\arraybackslash}p{15em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}>{\raggedright\arraybackslash}p{1em}}
		\toprule
		\rotatebox{0}{name} & \rotatebox{90}{$\textit{cor}_{\textit{dist}}$} & \rotatebox{90}{$\textit{NMSE}_{\textit{rf}}$} & \rotatebox{90}{$\textit{NMSE}_{\textit{lm}}$} & \rotatebox{90}{$\textit{edgeflip}$} & \rotatebox{90}{$\textit{HIM}$} & \rotatebox{90}{isomorphic} & \rotatebox{90}{$\textit{cor}_{\textit{features}}$} & \rotatebox{90}{$\textit{wcor}_{\textit{features}}$} & \rotatebox{90}{$\textit{F1}_{\textit{branches}}$} & \rotatebox{90}{$\textit{F1}_{\textit{milestones}}$} & \rotatebox{90}{$\textit{mean}_{\textit{geometric}}$}\\
		\midrule
		Same score on identity & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Local cell shuffling & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Edge shuffling & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Local and global cell shuffling & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Changing positions locally and/or globally & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		
		Cell filtering & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Removing divergence regions & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Move cells to start milestone & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Move cells to closest milestone & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Length shuffling & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		
		Cells into small subedges & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		New leaf edges & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		New connecting edges & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Changing topology and cell position & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Bifurcation merging & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		
		Bifurcation merging and changing cell positions & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Bifurcation concatentation & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Cycle breaking & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Linear joining & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Linear splitting & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		
		Change of topology & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{FB9A99}{\xmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		Cells on milestones vs edges & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}} & \multicolumn{1}{c}{\cellcolor[HTML]{B2DF8A}{\cmark}}\\
		\bottomrule
	\end{tabular}
	\endgroup{}
\end{table}

% \FloatBarrier

\subsection{Score aggregation}

To rank the methods, we need to aggregate on two levels: across \textbf{datasets} and across specific/application metrics to calculate an \textbf{overall metric}.

\subsubsection{Aggregating over datasets}

When combining different datasets, it is important that the biases in the datasets does not influence the overall score. In our study, we define three such biases, although there are potentially many more:

\begin{itemize}
	\item \textbf{Difficulty of the datasets} Some datasets are more difficult than others. This can have various reasons, such as the complexity of the topology, the amount of biological and technical noise, or the dimensions of the data. It is important that a small increase in performance on a more difficult dataset has an equal impact on the final score as a large increase in performance on easier datasets.
	\item \textbf{Dataset sources} It is much easier to generate synthetic datasets than real datasets, and this bias is reflected in our set of datasets. However, given their higher biological relevance, real datasets should be given at least equal importance than synthetic datasets.
	\item \textbf{Trajectory types} There are many more linear and disconnected real datasets, and only a limited number of tree or graph datasets. This imbalance is there because historically most datasets have been linear datasets, and because it is easy to create disconnected datasets by combining different datasets. However, this imbalance in trajectory types does not necessarily reflect the general importance of that trajectory type.
\end{itemize}

We designed an aggregation scheme which tries to prevent these biases from influencing the ranking of the methods.

The difficulty of a dataset can easily have an impact on how much weight the dataset gets in an overall ranking. We illustrate this with a simple example in Figure \ref{fig:snote1fig_11}. One method consistently performs well on both the easy and the difficult datasets. But because the differences are small in the difficult datasets, the mean would not give this method a high score. Meanwhile, a variable method which does not perform well on the difficult dataset gets the highest score, because it scored so high on the easier dataset.

To avoid this bias, we normalise the scores of each dataset by first scaling and centering to $\mu = 0$ and $\sigma = 1$, and then moving the score values back to $[0, 1]$ by applying the unit normal density distribution function. This results in scores which are comparable across different datasets (Figure \ref{fig:snote1fig_11}). In contrast to other possible normalisation techniques, this will still retain some information on the relative difference between the scores, which would have been lost when using the ranks for normalisation. An example of this normalisation, which will also be used in the subsequent aggregation steps, can be seen in Figure \ref{fig:snote1fig_12}.

\begin{figure}[htb!]
	\centering\includegraphics[width=\linewidth]{fig/snote1fig_11.pdf}
	\caption{
		\textbf{An illustration of how the difficulty of a dataset can influence the overall ranking.} 
		A decent method, which consistently ranks high on an easy and difficult dataset, does not get a high score when averaging. On the other hand, a method which ranks high on the easy dataset, but very low on the difficult dataset does get a high score on average. After normalising the scores (right), this problem disappears.
	}
	\label{fig:snote1fig_11}
\end{figure}

\begin{figure}[htb!]
	\centering\includegraphics[width=0.8\linewidth]{fig/snote1fig_12.pdf}
	\caption{
		\textbf{An example of the normalisation procedure.} 
		Shown are some results of a benchmarking procedure, where every row contains the scores of a particular method (red shading) on a particular dataset (blue shading), with a trajectory type (green shading) and dataset source (orange shading).
	}
	\label{fig:snote1fig_12}
\end{figure}

After normalisation, we aggregate step by step the scores from different datasets. We first aggregate the datasets with the same dataset source and trajectory type using an arithmetic mean of their scores (Figure \ref{fig:snote1fig_13}a). Next, the scores are averaged over different dataset sources, using a arithmetic mean which was weighted based on how much the synthetic and silver scores correlated with the real gold scores (Figure \ref{fig:snote1fig_13}b). Finally, the scores are aggregated over the different trajectory types again using a arithmetic mean (Figure \ref{fig:snote1fig_13}c).

\begin{figure}[htb!]
	\centering\includegraphics[width=0.8\linewidth]{fig/snote1fig_13.pdf}
	\caption{
		\textbf{An example of the aggregation procedure.} 
		In consecutive steps we aggregated across \textbf{(a)} different datasets with the same source and trajectory type, \textbf{(b)} different dataset sources with the same trajectory type (weighted for the correlation of the dataset source with the real gold dataset source) and \textbf{(c)} all trajectory types.
	}
	\label{fig:snote1fig_13}
\end{figure}

\subsubsection{Overall metrics}

Undoubtedly, a single optimal overall metric does not exist for trajectories, as different users may have different priorities:

\begin{itemize}
	\item A user may be primarily interested in defining the correct topology, and only use the cellular ordering when the topology is correct
	\item A user may be less interested in how the cells are ordered within a branch, but primarily in which cells are in which branches
	\item A user may already know the topology, and may be primarily interested in finding good features related to a particular branching point
	\item ...
\end{itemize}

Each of these scenarios would require a combinations of \textit{specific} and \textit{application} metrics with different weights. To provide an "overall" ranking of the metrics, which is impartial for the scenarios described above, we therefore chose a metric which weighs every aspect of the trajectory equally:

\begin{itemize}
	\item Its \textbf{ordering}, using the $\textit{cor}_{\textit{dist}}$
	\item Its \textbf{branch assignment}, using the $\textit{F1}_{\textit{branches}}$
	\item Its \textbf{topology}, using the $\textit{HIM}$
	\item The accuracy of \textbf{differentially expressed features}, using the $\textit{wcor}_{\textit{features}}$
\end{itemize}

Next, we considered three different ways of averaging different scores: the arithmetic mean, geometric mean and harmonic mean. Each of these types of mean have different use cases. The harmonic mean is most appropriate when the scores would all have a common denominator (as is the case for the $\textit{Recovery}$ and $\textit{Relevance}$ described earlier). The arithmetic mean would be most appropriate when all the metrics have the same range. For our use case, the geometric mean is the most appropriate, because it is low if one of the values is low. For example, this means that if a method is not good at inferring the correct topology, it will get a low overall score, even if it performs better at all other scores. This ensures that a high score will only be reached if a prediction has a good ordering, branch assignment, topology, and set of differentially expressed features.

The final overall score (Figure \ref{fig:snote1fig_14}) for a method was thus defined as: 

$$\textit{Overall} = \textit{mean}_{\textit{geometric}} = \sqrt[4]{\textit{cor}_{\textit{dist}} \times \textit{F1}_{\textit{branches}} \times \textit{HIM} \times \textit{wcor}_{\textit{features}}}$$

\begin{figure}[htb!]
	\centering\includegraphics[width=0.8\linewidth]{fig/snote1fig_14.pdf}
	\caption{
		\textbf{An example of the averaging procedure.} 
		For each method, we calculated the geometric mean between its normalised and aggregated scores
	}
	\label{fig:snote1fig_14}
\end{figure}

We do however want to stress that different use cases will require a different overall score to order the methods. Such a context-dependent ranking of all methods is provided through the dynguidelines app (\href{http://guidelines.dynverse.org}{http://guidelines.dynverse.org}).

\clearpage
\section{References}
\printbibliography[heading=none]